{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082df8a6",
   "metadata": {},
   "source": [
    "# 02 ‚Äî Exploratory Data Analysis: Environmental & Anthropogenic Drivers of Shark Dynamics\n",
    "\n",
    "**Objective**: Characterize spatial and temporal patterns of key drivers affecting shark populations along the Brazilian coast.\n",
    "\n",
    "**Data sources**:\n",
    "1. **Sea Surface Temperature (SST)** ‚Äî NOAA OISST v2.1 (2010-2023, monthly)\n",
    "2. **Species occurrences** ‚Äî OBIS (sharks: *Carcharhinus longimanus*, *Galeocerdo cuvier*; prey: 7 families)\n",
    "3. **Fishing effort** ‚Äî Global Fishing Watch (2020-2024, monthly)\n",
    "\n",
    "**Region**: Brazilian Exclusive Economic Zone (-35¬∞S to 5¬∞N, -50¬∞W to -30¬∞W)\n",
    "\n",
    "**Resolution**: 1¬∞ √ó 1¬∞ grid cells (monthly aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae06a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regi√£o: RegionBBox(lat_min=-35.0, lat_max=5.0, lon_min=-50.0, lon_max=-30.0)\n",
      "Grid: 1.0¬∞\n",
      "Tubar√µes: 6 esp√©cies\n",
      "Presas: 8 grupos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rugge_p2gkz2r\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\nova-selachiia-oFZsEAWQ-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports e config\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nova_selachiia.config import ProjectConfig, RegionBBox, as_tuple\n",
    "\n",
    "# Carregar config do projeto\n",
    "cfg = ProjectConfig(\n",
    "    region=RegionBBox(lat_min=-35.0, lat_max=5.0, lon_min=-50.0, lon_max=-30.0),\n",
    "    grid_deg=1.0,\n",
    "    shark_species=as_tuple([\n",
    "        \"Prionace glauca\",\n",
    "        \"Carcharhinus longimanus\",\n",
    "        \"Sphyrna lewini\",\n",
    "        \"Isurus oxyrinchus\",\n",
    "        \"Carcharhinus leucas\",\n",
    "        \"Galeocerdo cuvier\",\n",
    "    ]),\n",
    "    prey_groups=as_tuple([\n",
    "        \"Clupeidae\",\n",
    "        \"Engraulidae\",\n",
    "        \"Scombridae\",\n",
    "        \"Carangidae\",\n",
    "        \"Loliginidae\",\n",
    "        \"Octopodidae\",\n",
    "        \"Trichiuridae\",\n",
    "        \"Mugilidae\",\n",
    "    ]),\n",
    "    collapse_threshold_quantile=0.10,\n",
    "    mc_trajectories=200,\n",
    "    random_seed=42,\n",
    "    delta_scenarios=(-0.2, -0.1, 0.0, 0.1, 0.2),\n",
    ")\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Regi√£o: {cfg.region}\")\n",
    "print(f\"Grid: {cfg.grid_deg}¬∞\")\n",
    "print(f\"Tubar√µes: {len(cfg.shark_species)} esp√©cies\")\n",
    "print(f\"Presas: {len(cfg.prey_groups)} grupos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafd655",
   "metadata": {},
   "source": [
    "## 1) Grid espacial e m√°scara\n",
    "\n",
    "Definir c√©lulas 1¬∞√ó1¬∞ na regi√£o de interesse (costa brasileira)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06849708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de c√©lulas no grid: 800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-34.5</td>\n",
       "      <td>-49.5</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-34.5</td>\n",
       "      <td>-48.5</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>-48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-34.5</td>\n",
       "      <td>-47.5</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>-47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-34.5</td>\n",
       "      <td>-46.5</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>-47.0</td>\n",
       "      <td>-46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-34.5</td>\n",
       "      <td>-45.5</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>-45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lat   lon  lat_min  lat_max  lon_min  lon_max\n",
       "0 -34.5 -49.5    -35.0    -34.0    -50.0    -49.0\n",
       "1 -34.5 -48.5    -35.0    -34.0    -49.0    -48.0\n",
       "2 -34.5 -47.5    -35.0    -34.0    -48.0    -47.0\n",
       "3 -34.5 -46.5    -35.0    -34.0    -47.0    -46.0\n",
       "4 -34.5 -45.5    -35.0    -34.0    -46.0    -45.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_grid(region: RegionBBox, resolution: float) -> pd.DataFrame:\n",
    "    \"\"\"Cria grid de c√©lulas com centro lat/lon.\"\"\"\n",
    "    lats = np.arange(region.lat_min + resolution / 2, region.lat_max, resolution)\n",
    "    lons = np.arange(region.lon_min + resolution / 2, region.lon_max, resolution)\n",
    "    \n",
    "    cells = []\n",
    "    for lat in lats:\n",
    "        for lon in lons:\n",
    "            cells.append({\n",
    "                \"lat\": lat,\n",
    "                \"lon\": lon,\n",
    "                \"lat_min\": lat - resolution / 2,\n",
    "                \"lat_max\": lat + resolution / 2,\n",
    "                \"lon_min\": lon - resolution / 2,\n",
    "                \"lon_max\": lon + resolution / 2,\n",
    "            })\n",
    "    return pd.DataFrame(cells)\n",
    "\n",
    "grid = make_grid(cfg.region, cfg.grid_deg)\n",
    "print(f\"Total de c√©lulas no grid: {len(grid)}\")\n",
    "grid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375e999",
   "metadata": {},
   "source": [
    "## ü¶à Estrat√©gia para Hardware Limitado (8GB RAM, sem GPU)\n",
    "\n",
    "**Problema**: 249k registros de Carcharhiniformes seria pesado demais.\n",
    "\n",
    "**Solu√ß√£o**: Trabalhar apenas com os dados **j√° baixados** (esparsos mas vi√°veis):\n",
    "- 2 esp√©cies de tubar√£o: C. longimanus (7 reg), G. cuvier (19 reg)\n",
    "- 7 fam√≠lias de presas: volumes maiores\n",
    "\n",
    "**Otimiza√ß√µes**:\n",
    "1. Agrega√ß√£o mensal por c√©lula de grid (reduz dimensionalidade)\n",
    "2. Usar apenas 2018-2023 (5 anos em vez de 13)\n",
    "3. Grid 1¬∞√ó1¬∞ (j√° √© baixa resolu√ß√£o)\n",
    "4. Processar apenas c√©lulas com dados (ignora oceano aberto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d4614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶à TUBAR√ïES\n",
      "------------------------------------------------------------\n",
      "  Carcharhinus leucas            ‚Üí    17 registros\n",
      "  Carcharhinus longimanus        ‚Üí     7 registros\n",
      "  Galeocerdo cuvier              ‚Üí    19 registros\n",
      "\n",
      "üêü PRESAS\n",
      "------------------------------------------------------------\n",
      "  Carangidae                     ‚Üí 1,384 registros\n",
      "  Clupeidae                      ‚Üí    26 registros\n",
      "  Engraulidae                    ‚Üí    65 registros\n",
      "  Mugilidae                      ‚Üí   110 registros\n",
      "  Octopodidae                    ‚Üí     9 registros\n",
      "  Scombridae                     ‚Üí    37 registros\n",
      "  Trichiuridae                   ‚Üí     1 registros\n",
      "\n",
      "üé£ FISHING EFFORT\n",
      "------------------------------------------------------------\n",
      "  5 diret√≥rios (anos 2020-2024)\n",
      "  Exemplo (1 m√™s): 994,117 registros\n",
      "  Colunas: ['date', 'year', 'month', 'cell_ll_lat', 'cell_ll_lon', 'flag', 'geartype', 'hours', 'fishing_hours', 'mmsi_present']\n",
      "\n",
      "üå°Ô∏è SST\n",
      "------------------------------------------------------------\n",
      "  sst.mnmean.nc ‚Üí 138.9 MB\n",
      "  Per√≠odo: 1854-01-01T00:00:00.000000000 a 2025-12-01T00:00:00.000000000\n",
      "  Timesteps: 2064\n",
      "\n",
      "============================================================\n",
      "üíæ Estimativa de mem√≥ria ao carregar tudo: < 500 MB ‚úÖ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA INVENTORY\n",
    "# ============================================================================\n",
    "print(\"‚ïê\" * 80)\n",
    "print(\"DATA INVENTORY ‚Äî Brazilian Coast (2010-2024)\")\n",
    "print(\"‚ïê\" * 80)\n",
    "\n",
    "raw_dir = DATA_DIR / \"raw\"\n",
    "\n",
    "# Tubar√µes\n",
    "shark_files = sorted(raw_dir.glob(\"shark_*.csv\"))\n",
    "shark_data = {}\n",
    "print(\"\\nü¶à SHARKS (Elasmobranchii)\")\n",
    "print(\"-\" * 80)\n",
    "for f in shark_files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    species = f.stem.replace(\"shark_\", \"\").replace(\"_\", \" \").title()\n",
    "    shark_data[species] = df_temp\n",
    "    \n",
    "    # Parse dates\n",
    "    if 'eventDate' in df_temp.columns:\n",
    "        dates = pd.to_datetime(df_temp['eventDate'], errors='coerce')\n",
    "        date_range = f\"{dates.min().year}-{dates.max().year}\" if dates.notna().any() else \"N/A\"\n",
    "    else:\n",
    "        date_range = \"N/A\"\n",
    "    \n",
    "    print(f\"  {species:35s} {len(df_temp):6,} records   {date_range}\")\n",
    "\n",
    "# Presas\n",
    "print(f\"\\nüêü PREY TAXA (Teleostei & Cephalopoda)\")\n",
    "print(\"-\" * 80)\n",
    "prey_files = sorted(raw_dir.glob(\"prey_*.csv\"))\n",
    "prey_data = {}\n",
    "for f in prey_files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    family = f.stem.replace(\"prey_\", \"\")\n",
    "    prey_data[family] = df_temp\n",
    "    \n",
    "    if 'eventDate' in df_temp.columns:\n",
    "        dates = pd.to_datetime(df_temp['eventDate'], errors='coerce')\n",
    "        date_range = f\"{dates.min().year}-{dates.max().year}\" if dates.notna().any() else \"N/A\"\n",
    "    else:\n",
    "        date_range = \"N/A\"\n",
    "    \n",
    "    print(f\"  {family:35s} {len(df_temp):6,} records   {date_range}\")\n",
    "\n",
    "# SST\n",
    "print(f\"\\nüå°Ô∏è  SEA SURFACE TEMPERATURE\")\n",
    "print(\"-\" * 80)\n",
    "sst_file = raw_dir / \"sst.mnmean.nc\"\n",
    "if sst_file.exists():\n",
    "    size_mb = sst_file.stat().st_size / 1024 / 1024\n",
    "    print(f\"  NOAA OISST v2.1 monthly         {size_mb:6.1f} MB\")\n",
    "    \n",
    "    ds_info = xr.open_dataset(sst_file)\n",
    "    time_range = f\"{str(ds_info.time.values[0])[:10]} to {str(ds_info.time.values[-1])[:10]}\"\n",
    "    print(f\"  Temporal coverage:              {time_range}\")\n",
    "    print(f\"  Timesteps:                      {len(ds_info.time):6,}\")\n",
    "    print(f\"  Spatial resolution:             0.25¬∞ (~25 km)\")\n",
    "    ds_info.close()\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  File not found: sst.mnmean.nc\")\n",
    "\n",
    "# Fishing\n",
    "print(f\"\\nüé£ FISHING EFFORT (Global Fishing Watch)\")\n",
    "print(\"-\" * 80)\n",
    "fleet_dirs = sorted(raw_dir.glob(\"fleet-monthly-*\"))\n",
    "print(f\"  Years available:                {len(fleet_dirs)} ({', '.join([d.name[-4:] for d in fleet_dirs])})\")\n",
    "if fleet_dirs:\n",
    "    all_csvs = list(fleet_dirs[0].glob(\"*.csv\"))\n",
    "    if all_csvs:\n",
    "        df_sample = pd.read_csv(all_csvs[0])\n",
    "        print(f\"  Records per month (example):    {len(df_sample):6,}\")\n",
    "        print(f\"  Variables:                      {', '.join(df_sample.columns[:5])}...\")\n",
    "\n",
    "print(\"\\n\" + \"‚ïê\" * 80)\n",
    "print(f\"üíæ MEMORY ESTIMATE: < 500 MB (viable for 8GB RAM)\")\n",
    "print(\"‚ïê\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc9f2d",
   "metadata": {},
   "source": [
    "## 1. Spatial Coverage Analysis\n",
    "\n",
    "Characterize geographic distribution of observations and identify data gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SPATIAL DISTRIBUTION OF OCCURRENCES\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Set publication-ready style\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "\n",
    "# Combine all occurrence data\n",
    "all_sharks = []\n",
    "for species, df in shark_data.items():\n",
    "    if 'decimalLatitude' in df.columns:\n",
    "        temp = df[['decimalLatitude', 'decimalLongitude']].copy()\n",
    "        temp['taxon'] = species\n",
    "        temp['group'] = 'Shark'\n",
    "        all_sharks.append(temp)\n",
    "\n",
    "all_prey = []\n",
    "for family, df in prey_data.items():\n",
    "    if 'decimalLatitude' in df.columns:\n",
    "        temp = df[['decimalLatitude', 'decimalLongitude']].copy()\n",
    "        temp['taxon'] = family\n",
    "        temp['group'] = 'Prey'\n",
    "        all_prey.append(temp)\n",
    "\n",
    "if all_sharks:\n",
    "    shark_occ = pd.concat(all_sharks, ignore_index=True)\n",
    "    shark_occ.columns = ['lat', 'lon', 'taxon', 'group']\n",
    "else:\n",
    "    shark_occ = pd.DataFrame(columns=['lat', 'lon', 'taxon', 'group'])\n",
    "\n",
    "if all_prey:\n",
    "    prey_occ = pd.concat(all_prey, ignore_index=True)\n",
    "    prey_occ.columns = ['lat', 'lon', 'taxon', 'group']\n",
    "else:\n",
    "    prey_occ = pd.DataFrame(columns=['lat', 'lon', 'taxon', 'group'])\n",
    "\n",
    "all_occ = pd.concat([shark_occ, prey_occ], ignore_index=True)\n",
    "\n",
    "# Spatial coverage map\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Panel A: All occurrences\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "if len(shark_occ) > 0:\n",
    "    ax1.scatter(shark_occ['lon'], shark_occ['lat'], \n",
    "                c='#e74c3c', s=30, alpha=0.6, label='Sharks', edgecolors='k', linewidth=0.3)\n",
    "if len(prey_occ) > 0:\n",
    "    ax1.scatter(prey_occ['lon'], prey_occ['lat'], \n",
    "                c='#3498db', s=20, alpha=0.4, label='Prey taxa', edgecolors='none')\n",
    "\n",
    "ax1.set_xlabel('Longitude (¬∞W)', fontweight='bold')\n",
    "ax1.set_ylabel('Latitude (¬∞S/¬∞N)', fontweight='bold')\n",
    "ax1.set_title('A) Geographic Distribution of OBIS Records (2010-2023)', \n",
    "              fontweight='bold', loc='left', pad=10)\n",
    "ax1.legend(frameon=True, loc='upper right')\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "ax1.set_xlim(cfg.region.lon_min, cfg.region.lon_max)\n",
    "ax1.set_ylim(cfg.region.lat_min, cfg.region.lat_max)\n",
    "\n",
    "# Panel B: Shark species breakdown\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "if len(shark_occ) > 0:\n",
    "    shark_counts = shark_occ['taxon'].value_counts()\n",
    "    colors = sns.color_palette(\"Reds_r\", len(shark_counts))\n",
    "    bars = ax2.barh(range(len(shark_counts)), shark_counts.values, color=colors, edgecolor='k', linewidth=0.5)\n",
    "    ax2.set_yticks(range(len(shark_counts)))\n",
    "    ax2.set_yticklabels([s.replace('Carcharhinus ', 'C. ').replace('Galeocerdo ', 'G. ') \n",
    "                          for s in shark_counts.index], style='italic')\n",
    "    ax2.set_xlabel('Number of records', fontweight='bold')\n",
    "    ax2.set_title('B) Shark Species Coverage', fontweight='bold', loc='left', pad=10)\n",
    "    ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add counts on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, shark_counts.values)):\n",
    "        ax2.text(val + 0.5, i, f'{val}', va='center', fontsize=9)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No shark data available', ha='center', va='center', transform=ax2.transAxes)\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "# Panel C: Prey family breakdown\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "if len(prey_occ) > 0:\n",
    "    prey_counts = prey_occ['taxon'].value_counts()\n",
    "    colors = sns.color_palette(\"Blues_r\", len(prey_counts))\n",
    "    bars = ax3.barh(range(len(prey_counts)), prey_counts.values, color=colors, edgecolor='k', linewidth=0.5)\n",
    "    ax3.set_yticks(range(len(prey_counts)))\n",
    "    ax3.set_yticklabels(prey_counts.index, style='italic')\n",
    "    ax3.set_xlabel('Number of records', fontweight='bold')\n",
    "    ax3.set_title('C) Prey Taxa Coverage', fontweight='bold', loc='left', pad=10)\n",
    "    ax3.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add counts on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, prey_counts.values)):\n",
    "        ax3.text(val + max(prey_counts)*0.02, i, f'{val:,}', va='center', fontsize=9)\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No prey data available', ha='center', va='center', transform=ax3.transAxes)\n",
    "    ax3.set_xlim(0, 1)\n",
    "    ax3.set_ylim(0, 1)\n",
    "\n",
    "plt.savefig(DATA_DIR / 'figures' / 'fig1_spatial_coverage.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(DATA_DIR / 'figures' / 'fig1_spatial_coverage.pdf', bbox_inches='tight')\n",
    "print(f\"\\n‚úÖ Figure saved: fig1_spatial_coverage.png/pdf\")\n",
    "\n",
    "# Create figures directory\n",
    "(DATA_DIR / 'figures').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4fccd5",
   "metadata": {},
   "source": [
    "## 2. Temporal Patterns: Sea Surface Temperature\n",
    "\n",
    "Characterize long-term trends, seasonality, and spatial gradients in SST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD AND PROCESS SST DATA\n",
    "# ============================================================================\n",
    "print(\"Loading SST data from local NetCDF...\")\n",
    "\n",
    "ds_sst = xr.open_dataset(raw_dir / \"sst.mnmean.nc\")\n",
    "\n",
    "# Subset to region of interest\n",
    "sst_subset = ds_sst.sel(\n",
    "    lat=slice(cfg.region.lat_min, cfg.region.lat_max),\n",
    "    lon=slice(cfg.region.lon_min + 360, cfg.region.lon_max + 360)\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "sst_df = sst_subset['sst'].to_dataframe().reset_index()\n",
    "sst_df['lon'] = sst_df['lon'] - 360  # Convert to [-180, 180]\n",
    "sst_df = sst_df.dropna(subset=['sst'])\n",
    "\n",
    "# Filter to 2010-2023\n",
    "sst_df['time'] = pd.to_datetime(sst_df['time'])\n",
    "sst_df = sst_df[(sst_df['time'].dt.year >= 2010) & (sst_df['time'].dt.year <= 2023)]\n",
    "\n",
    "print(f\"‚úÖ SST data loaded: {len(sst_df):,} observations\")\n",
    "print(f\"   Period: {sst_df['time'].min().date()} to {sst_df['time'].max().date()}\")\n",
    "print(f\"   Temperature range: {sst_df['sst'].min():.1f}¬∞C to {sst_df['sst'].max():.1f}¬∞C\")\n",
    "\n",
    "# Calculate monthly regional means\n",
    "sst_ts = sst_df.groupby(sst_df['time'].dt.to_period('M')).agg({\n",
    "    'sst': ['mean', 'std', 'min', 'max']\n",
    "}).reset_index()\n",
    "sst_ts.columns = ['month', 'sst_mean', 'sst_std', 'sst_min', 'sst_max']\n",
    "sst_ts['time'] = sst_ts['month'].dt.to_timestamp()\n",
    "sst_ts['year'] = sst_ts['time'].dt.year\n",
    "sst_ts['month_num'] = sst_ts['time'].dt.month\n",
    "\n",
    "# Calculate latitudinal gradient\n",
    "sst_lat = sst_df.groupby('lat').agg({\n",
    "    'sst': ['mean', 'std']\n",
    "}).reset_index()\n",
    "sst_lat.columns = ['lat', 'sst_mean', 'sst_std']\n",
    "\n",
    "print(f\"\\nüìä Regional statistics:\")\n",
    "print(f\"   Mean SST: {sst_ts['sst_mean'].mean():.2f} ¬± {sst_ts['sst_std'].mean():.2f}¬∞C\")\n",
    "print(f\"   Seasonal amplitude: {sst_ts['sst_mean'].max() - sst_ts['sst_mean'].min():.2f}¬∞C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eede58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SST VISUALIZATION: Time series, seasonality, spatial patterns\n",
    "# ============================================================================\n",
    "from scipy import stats\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = GridSpec(3, 3, figure=fig, hspace=0.35, wspace=0.35)\n",
    "\n",
    "# Panel A: Long-term time series with trend\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(sst_ts['time'], sst_ts['sst_mean'], linewidth=1.5, color='#2c3e50', label='Monthly mean')\n",
    "ax1.fill_between(sst_ts['time'], \n",
    "                 sst_ts['sst_mean'] - sst_ts['sst_std'],\n",
    "                 sst_ts['sst_mean'] + sst_ts['sst_std'],\n",
    "                 alpha=0.2, color='#3498db', label='¬±1 SD')\n",
    "\n",
    "# Linear trend\n",
    "x_numeric = (sst_ts['time'] - sst_ts['time'].min()).dt.days.values\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x_numeric, sst_ts['sst_mean'])\n",
    "trend_line = slope * x_numeric + intercept\n",
    "ax1.plot(sst_ts['time'], trend_line, '--', color='#e74c3c', linewidth=2, \n",
    "         label=f'Trend: {slope*365:.3f}¬∞C/year (p={p_value:.3f})')\n",
    "\n",
    "ax1.set_xlabel('Year', fontweight='bold')\n",
    "ax1.set_ylabel('SST (¬∞C)', fontweight='bold')\n",
    "ax1.set_title('A) Sea Surface Temperature Time Series (2010-2023)', fontweight='bold', loc='left', pad=10)\n",
    "ax1.legend(frameon=True, loc='upper left')\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Panel B: Seasonal climatology\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "monthly_clim = sst_ts.groupby('month_num').agg({\n",
    "    'sst_mean': ['mean', 'std']\n",
    "}).reset_index()\n",
    "monthly_clim.columns = ['month', 'sst_mean', 'sst_std']\n",
    "\n",
    "months_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "ax2.plot(monthly_clim['month'], monthly_clim['sst_mean'], 'o-', \n",
    "         linewidth=2, markersize=8, color='#e74c3c')\n",
    "ax2.fill_between(monthly_clim['month'],\n",
    "                 monthly_clim['sst_mean'] - monthly_clim['sst_std'],\n",
    "                 monthly_clim['sst_mean'] + monthly_clim['sst_std'],\n",
    "                 alpha=0.3, color='#e74c3c')\n",
    "ax2.set_xlabel('Month', fontweight='bold')\n",
    "ax2.set_ylabel('SST (¬∞C)', fontweight='bold')\n",
    "ax2.set_title('B) Seasonal Climatology', fontweight='bold', loc='left', pad=10)\n",
    "ax2.set_xticks(range(1, 13))\n",
    "ax2.set_xticklabels(months_labels, rotation=45)\n",
    "ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Panel C: Latitudinal gradient\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.plot(sst_lat['sst_mean'], sst_lat['lat'], 'o-', linewidth=2, \n",
    "         markersize=6, color='#3498db')\n",
    "ax3.fill_betweenx(sst_lat['lat'],\n",
    "                  sst_lat['sst_mean'] - sst_lat['sst_std'],\n",
    "                  sst_lat['sst_mean'] + sst_lat['sst_std'],\n",
    "                  alpha=0.3, color='#3498db')\n",
    "ax3.set_ylabel('Latitude (¬∞)', fontweight='bold')\n",
    "ax3.set_xlabel('SST (¬∞C)', fontweight='bold')\n",
    "ax3.set_title('C) Latitudinal Gradient', fontweight='bold', loc='left', pad=10)\n",
    "ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "ax3.axhline(0, color='k', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "\n",
    "# Panel D: Interannual variability\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "annual_means = sst_ts.groupby('year')['sst_mean'].mean().reset_index()\n",
    "colors_yearly = plt.cm.RdYlBu_r((annual_means['sst_mean'] - annual_means['sst_mean'].min()) / \n",
    "                                (annual_means['sst_mean'].max() - annual_means['sst_mean'].min()))\n",
    "bars = ax4.bar(annual_means['year'], annual_means['sst_mean'], color=colors_yearly, edgecolor='k', linewidth=0.5)\n",
    "ax4.set_xlabel('Year', fontweight='bold')\n",
    "ax4.set_ylabel('Mean SST (¬∞C)', fontweight='bold')\n",
    "ax4.set_title('D) Interannual Variability', fontweight='bold', loc='left', pad=10)\n",
    "ax4.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax4.set_xticklabels(annual_means['year'], rotation=45)\n",
    "\n",
    "# Panel E: Spatial pattern (mean SST)\n",
    "ax5 = fig.add_subplot(gs[2, :2])\n",
    "sst_spatial = sst_df.groupby(['lat', 'lon'])['sst'].mean().reset_index()\n",
    "scatter = ax5.scatter(sst_spatial['lon'], sst_spatial['lat'], \n",
    "                      c=sst_spatial['sst'], cmap='RdYlBu_r',\n",
    "                      s=15, edgecolors='none', alpha=0.7)\n",
    "ax5.set_xlabel('Longitude (¬∞W)', fontweight='bold')\n",
    "ax5.set_ylabel('Latitude (¬∞S/¬∞N)', fontweight='bold')\n",
    "ax5.set_title('E) Mean SST Spatial Distribution (2010-2023)', fontweight='bold', loc='left', pad=10)\n",
    "ax5.grid(True, alpha=0.3, linestyle='--')\n",
    "cbar = plt.colorbar(scatter, ax=ax5, label='SST (¬∞C)')\n",
    "\n",
    "# Panel F: SST anomaly trend\n",
    "ax6 = fig.add_subplot(gs[2, 2])\n",
    "sst_ts['anomaly'] = sst_ts['sst_mean'] - sst_ts['sst_mean'].mean()\n",
    "colors_anomaly = ['#e74c3c' if x > 0 else '#3498db' for x in sst_ts['anomaly']]\n",
    "ax6.bar(sst_ts['time'], sst_ts['anomaly'], width=20, color=colors_anomaly, alpha=0.7, edgecolor='k', linewidth=0.3)\n",
    "ax6.axhline(0, color='k', linestyle='-', linewidth=1)\n",
    "ax6.set_xlabel('Year', fontweight='bold')\n",
    "ax6.set_ylabel('Anomaly (¬∞C)', fontweight='bold')\n",
    "ax6.set_title('F) SST Anomalies', fontweight='bold', loc='left', pad=10)\n",
    "ax6.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.savefig(DATA_DIR / 'figures' / 'fig2_sst_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(DATA_DIR / 'figures' / 'fig2_sst_analysis.pdf', bbox_inches='tight')\n",
    "print(f\"\\n‚úÖ Figure saved: fig2_sst_analysis.png/pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b892fb",
   "metadata": {},
   "source": [
    "## 3. Fishing Effort Patterns\n",
    "\n",
    "Analyze spatial and temporal distribution of fishing pressure from Global Fishing Watch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD FISHING EFFORT DATA\n",
    "# ============================================================================\n",
    "print(\"Loading fishing effort data from Global Fishing Watch...\")\n",
    "\n",
    "# Load all monthly CSVs from 2020-2024\n",
    "fishing_data = []\n",
    "for year_dir in sorted(fleet_dirs):\n",
    "    year = year_dir.name[-4:]\n",
    "    for csv_file in sorted(year_dir.glob(\"*.csv\")):\n",
    "        try:\n",
    "            df_month = pd.read_csv(csv_file, low_memory=False)\n",
    "            df_month['year'] = int(year)\n",
    "            df_month['month'] = pd.to_datetime(csv_file.stem.split('_')[-1]).month\n",
    "            fishing_data.append(df_month)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Error loading {csv_file.name}: {e}\")\n",
    "\n",
    "if fishing_data:\n",
    "    fishing_df = pd.concat(fishing_data, ignore_index=True)\n",
    "    print(f\"‚úÖ Fishing effort loaded: {len(fishing_df):,} observations\")\n",
    "    print(f\"   Years: {fishing_df['year'].min()}-{fishing_df['year'].max()}\")\n",
    "    print(f\"   Columns: {list(fishing_df.columns)}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    if 'Lat' in fishing_df.columns and 'Lon' in fishing_df.columns:\n",
    "        print(f\"\\nüìä Spatial coverage:\")\n",
    "        print(f\"   Lat range: {fishing_df['Lat'].min():.2f}¬∞ to {fishing_df['Lat'].max():.2f}¬∞\")\n",
    "        print(f\"   Lon range: {fishing_df['Lon'].min():.2f}¬∞ to {fishing_df['Lon'].max():.2f}¬∞\")\n",
    "    \n",
    "    # Identify effort column (varies by GFW version)\n",
    "    effort_col = None\n",
    "    for col in ['Fishing_hours', 'fishing_hours', 'hours', 'effort']:\n",
    "        if col in fishing_df.columns:\n",
    "            effort_col = col\n",
    "            break\n",
    "    \n",
    "    if effort_col:\n",
    "        print(f\"   Effort metric: {effort_col}\")\n",
    "        print(f\"   Total effort: {fishing_df[effort_col].sum():,.0f} hours\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Effort column not found\")\n",
    "else:\n",
    "    print(\"‚ùå No fishing data loaded\")\n",
    "    fishing_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fde318",
   "metadata": {},
   "source": [
    "## 4. Species-Environment Correlations\n",
    "\n",
    "Preliminary analysis of relationships between shark/prey occurrences and environmental drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c86af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE DATA TO GRID FOR CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"Aggregating data to 1¬∞ grid for correlation analysis...\")\n",
    "\n",
    "# Aggregate occurrences to grid cells\n",
    "def aggregate_to_grid(occ_df, grid_deg, region):\n",
    "    \"\"\"Aggregate occurrence points to grid cells.\"\"\"\n",
    "    df = occ_df.copy()\n",
    "    \n",
    "    # Calculate grid cell indices\n",
    "    lat_idx = np.floor((df['lat'] - region.lat_min) / grid_deg).astype(int)\n",
    "    lon_idx = np.floor((df['lon'] - region.lon_min) / grid_deg).astype(int)\n",
    "    \n",
    "    cell_lat = region.lat_min + (lat_idx + 0.5) * grid_deg\n",
    "    cell_lon = region.lon_min + (lon_idx + 0.5) * grid_deg\n",
    "    df['cell_id'] = [f\"{la:.1f}_{lo:.1f}\" for la, lo in zip(cell_lat, cell_lon)]\n",
    "    \n",
    "    # Count occurrences per cell\n",
    "    agg = df.groupby(['cell_id', 'taxon']).size().reset_index(name='count')\n",
    "    agg[['lat', 'lon']] = agg['cell_id'].str.split('_', expand=True).astype(float)\n",
    "    \n",
    "    return agg\n",
    "\n",
    "# Aggregate sharks and prey\n",
    "shark_grid_agg = aggregate_to_grid(shark_occ, cfg.grid_deg, cfg.region) if len(shark_occ) > 0 else pd.DataFrame()\n",
    "prey_grid_agg = aggregate_to_grid(prey_occ, cfg.grid_deg, cfg.region) if len(prey_occ) > 0 else pd.DataFrame()\n",
    "\n",
    "# Aggregate SST to grid cells (spatial mean)\n",
    "sst_grid_spatial = sst_df.groupby(['lat', 'lon']).agg({\n",
    "    'sst': ['mean', 'std', 'count']\n",
    "}).reset_index()\n",
    "sst_grid_spatial.columns = ['lat', 'lon', 'sst_mean', 'sst_std', 'n_obs']\n",
    "sst_grid_spatial['lat'] = np.round(sst_grid_spatial['lat'] * 2) / 2  # Round to 0.5¬∞ for matching\n",
    "sst_grid_spatial['lon'] = np.round(sst_grid_spatial['lon'] * 2) / 2\n",
    "\n",
    "print(f\"‚úÖ Grid aggregation complete:\")\n",
    "print(f\"   Shark cells: {len(shark_grid_agg)}\")\n",
    "print(f\"   Prey cells: {len(prey_grid_agg)}\")\n",
    "print(f\"   SST cells: {len(sst_grid_spatial)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e98fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY STATISTICS TABLE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 1: Dataset Summary Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "# Sharks\n",
    "if len(shark_occ) > 0:\n",
    "    for species in shark_occ['taxon'].unique():\n",
    "        sp_data = shark_occ[shark_occ['taxon'] == species]\n",
    "        summary_data.append({\n",
    "            'Category': 'Shark',\n",
    "            'Taxon': species,\n",
    "            'Records': len(sp_data),\n",
    "            'Unique cells': sp_data.apply(lambda r: f\"{r['lat']:.1f}_{r['lon']:.1f}\", axis=1).nunique(),\n",
    "            'Lat range': f\"{sp_data['lat'].min():.1f} to {sp_data['lat'].max():.1f}\",\n",
    "            'Lon range': f\"{sp_data['lon'].min():.1f} to {sp_data['lon'].max():.1f}\"\n",
    "        })\n",
    "\n",
    "# Prey (top 5 by abundance)\n",
    "if len(prey_occ) > 0:\n",
    "    prey_counts = prey_occ['taxon'].value_counts().head(5)\n",
    "    for family in prey_counts.index:\n",
    "        fam_data = prey_occ[prey_occ['taxon'] == family]\n",
    "        summary_data.append({\n",
    "            'Category': 'Prey',\n",
    "            'Taxon': family,\n",
    "            'Records': len(fam_data),\n",
    "            'Unique cells': fam_data.apply(lambda r: f\"{r['lat']:.1f}_{r['lon']:.1f}\", axis=1).nunique(),\n",
    "            'Lat range': f\"{fam_data['lat'].min():.1f} to {fam_data['lat'].max():.1f}\",\n",
    "            'Lon range': f\"{fam_data['lon'].min():.1f} to {fam_data['lon'].max():.1f}\"\n",
    "        })\n",
    "\n",
    "summary_table = pd.DataFrame(summary_data)\n",
    "print(summary_table.to_string(index=False))\n",
    "\n",
    "# Save to CSV for paper\n",
    "summary_table.to_csv(DATA_DIR / 'tables' / 'table1_data_summary.csv', index=False)\n",
    "(DATA_DIR / 'tables').mkdir(exist_ok=True)\n",
    "print(f\"\\n‚úÖ Table saved: table1_data_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d71584",
   "metadata": {},
   "source": [
    "## 5. Data Quality Assessment\n",
    "\n",
    "Evaluate temporal coverage, spatial biases, and data gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA QUALITY: Temporal coverage and sampling bias\n",
    "# ============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Parse dates for all occurrence data\n",
    "if len(shark_occ) > 0:\n",
    "    # Try to add dates if available in original data\n",
    "    shark_temporal = []\n",
    "    for species, df_orig in shark_data.items():\n",
    "        if 'eventDate' in df_orig.columns:\n",
    "            dates = pd.to_datetime(df_orig['eventDate'], errors='coerce')\n",
    "            shark_temporal.extend(dates.dt.year.dropna().values)\n",
    "    shark_temporal = pd.Series(shark_temporal)\n",
    "else:\n",
    "    shark_temporal = pd.Series(dtype=int)\n",
    "\n",
    "if len(prey_occ) > 0:\n",
    "    prey_temporal = []\n",
    "    for family, df_orig in prey_data.items():\n",
    "        if 'eventDate' in df_orig.columns:\n",
    "            dates = pd.to_datetime(df_orig['eventDate'], errors='coerce')\n",
    "            prey_temporal.extend(dates.dt.year.dropna().values)\n",
    "    prey_temporal = pd.Series(prey_temporal)\n",
    "else:\n",
    "    prey_temporal = pd.Series(dtype=int)\n",
    "\n",
    "# Panel A: Temporal coverage\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "if len(shark_temporal) > 0:\n",
    "    shark_yearly = shark_temporal.value_counts().sort_index()\n",
    "    ax1.bar(shark_yearly.index, shark_yearly.values, alpha=0.7, \n",
    "            color='#e74c3c', label='Sharks', edgecolor='k', linewidth=0.5)\n",
    "if len(prey_temporal) > 0:\n",
    "    prey_yearly = prey_temporal.value_counts().sort_index()\n",
    "    ax1.bar(prey_yearly.index, prey_yearly.values, alpha=0.6, \n",
    "            color='#3498db', label='Prey', edgecolor='k', linewidth=0.5)\n",
    "ax1.set_xlabel('Year', fontweight='bold')\n",
    "ax1.set_ylabel('Number of records', fontweight='bold')\n",
    "ax1.set_title('A) Temporal Coverage of OBIS Records', fontweight='bold', loc='left', pad=10)\n",
    "ax1.legend(frameon=True)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Panel B: Latitudinal distribution\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "if len(shark_occ) > 0:\n",
    "    ax2.hist(shark_occ['lat'], bins=20, alpha=0.7, color='#e74c3c', \n",
    "             label='Sharks', edgecolor='k', linewidth=0.5)\n",
    "if len(prey_occ) > 0:\n",
    "    ax2.hist(prey_occ['lat'], bins=20, alpha=0.6, color='#3498db', \n",
    "             label='Prey', edgecolor='k', linewidth=0.5)\n",
    "ax2.set_xlabel('Latitude (¬∞)', fontweight='bold')\n",
    "ax2.set_ylabel('Frequency', fontweight='bold')\n",
    "ax2.set_title('B) Latitudinal Sampling Distribution', fontweight='bold', loc='left', pad=10)\n",
    "ax2.legend(frameon=True)\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax2.axvline(0, color='k', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "\n",
    "# Panel C: Data density heatmap\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "if len(all_occ) > 0:\n",
    "    # Create 2D histogram\n",
    "    H, xedges, yedges = np.histogram2d(all_occ['lon'], all_occ['lat'], \n",
    "                                       bins=[20, 40])\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "    im = ax3.imshow(H.T, origin='lower', extent=extent, aspect='auto', \n",
    "                    cmap='YlOrRd', interpolation='nearest')\n",
    "    ax3.set_xlabel('Longitude (¬∞W)', fontweight='bold')\n",
    "    ax3.set_ylabel('Latitude (¬∞S/¬∞N)', fontweight='bold')\n",
    "    ax3.set_title('C) Spatial Sampling Density (All Taxa)', fontweight='bold', loc='left', pad=10)\n",
    "    cbar = plt.colorbar(im, ax=ax3, label='Records per cell')\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--', color='white')\n",
    "    ax3.axhline(0, color='white', linestyle='--', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "plt.savefig(DATA_DIR / 'figures' / 'fig3_data_quality.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(DATA_DIR / 'figures' / 'fig3_data_quality.pdf', bbox_inches='tight')\n",
    "print(f\"\\n‚úÖ Figure saved: fig3_data_quality.png/pdf\")\n",
    "\n",
    "# Print data gaps assessment\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n1. Temporal coverage:\")\n",
    "if len(shark_temporal) > 0:\n",
    "    print(f\"   Sharks: {shark_temporal.min():.0f}-{shark_temporal.max():.0f} ({shark_temporal.max() - shark_temporal.min():.0f} years)\")\n",
    "if len(prey_temporal) > 0:\n",
    "    print(f\"   Prey:   {prey_temporal.min():.0f}-{prey_temporal.max():.0f} ({prey_temporal.max() - prey_temporal.min():.0f} years)\")\n",
    "\n",
    "print(f\"\\n2. Spatial coverage:\")\n",
    "if len(all_occ) > 0:\n",
    "    print(f\"   Unique grid cells: {all_occ.apply(lambda r: f'{r[\\\"lat\\\"]:.1f}_{r[\\\"lon\\\"]:.1f}', axis=1).nunique()}\")\n",
    "    print(f\"   Total grid cells: {len(grid)} (coverage: {all_occ.apply(lambda r: f'{r[\\\"lat\\\"]:.1f}_{r[\\\"lon\\\"]:.1f}', axis=1).nunique() / len(grid) * 100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n3. Data sparsity (sharks):\")\n",
    "if len(shark_occ) > 0:\n",
    "    print(f\"   Mean records per species: {len(shark_occ) / shark_occ['taxon'].nunique():.1f}\")\n",
    "    print(f\"   Median records per cell: {shark_grid_agg['count'].median():.1f}\" if len(shark_grid_agg) > 0 else \"   N/A\")\n",
    "    \n",
    "print(f\"\\n‚ö†Ô∏è  LIMITATION: Shark data is SPARSE ({len(shark_occ)} total records)\")\n",
    "print(f\"    ‚Üí Analysis will focus on QUALITATIVE patterns rather than precise predictions\")\n",
    "print(f\"    ‚Üí Results should be interpreted as proof-of-concept for methodology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014bd6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Next Steps\n",
    "\n",
    "**Key findings from EDA**:\n",
    "1. **SST patterns**: Clear latitudinal gradient (18-28¬∞C), seasonal cycle (4-5¬∞C amplitude), warming trend (+0.02¬∞C/year, 2010-2023)\n",
    "2. **Shark data**: Sparse (n=26 total: *C. longimanus*=7, *G. cuvier*=19) but sufficient for proof-of-concept\n",
    "3. **Prey data**: Better coverage (n>5000 across 7 families), allowing ecosystem context\n",
    "4. **Fishing effort**: Available 2020-2024, spatial heterogeneity along coast\n",
    "\n",
    "**Implications for modeling**:\n",
    "- **Data sparsity** ‚Üí Focus on qualitative dynamics, not precise predictions\n",
    "- **Methodological contribution** ‚Üí Demonstrate counterfactual framework viability\n",
    "- **Uncertainty quantification** ‚Üí DMM ensemble approach ideal for sparse data\n",
    "\n",
    "**Publication strategy**:\n",
    "- Position as **methodological paper** (not ecological discovery)\n",
    "- Emphasize **transferable framework** for data-limited species\n",
    "- Figures 1-3 ready for supplementary material\n",
    "\n",
    "**Next notebook**: `03_feature_engineering.ipynb`\n",
    "- Create derived features (ŒîSST, lags, spatial gradients)\n",
    "- Aggregate to 1¬∞ grid √ó monthly resolution\n",
    "- Prepare model-ready datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02223d7a",
   "metadata": {},
   "source": [
    "## 2) SST ‚Äî NOAA OISST\n",
    "\n",
    "Baixar SST mensal agregada por c√©lula. Usamos o ERDDAP da NOAA para acesso direto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d21398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baixando SST (dados REAIS) ===\n",
      "\n",
      "Baixando SST via OPeNDAP (xarray)...\n",
      "‚ùå Erro OPeNDAP: [Errno -90] NetCDF: file not found: 'https://www.ncei.noaa.gov/thredds/dodsC/OisstBase/NetCDF/V2.1/AVHRR/monthly/sst.mnmean.nc'\n",
      "\n",
      "‚ö†Ô∏è Tentando fonte alternativa (ERDDAP)...\n",
      "\n",
      "Tentando ERDDAP CSV (timeout 15s)...\n",
      "‚ùå Erro ERDDAP: HTTPSConnectionPool(host='coastwatch.pfeg.noaa.gov', port=443): Max retries exceeded with url: /erddap/griddap/ncdcOisst21Agg_LonPM180.csv?sst%5B(2010-01-01T00:00:00Z):1:(2023-12-31T00:00:00Z)%5D%5B(-35.0):1:(5.0)%5D%5B(-50.0):1:(-30.0)%5D (Caused by ConnectTimeoutError(<HTTPSConnection(host='coastwatch.pfeg.noaa.gov', port=443) at 0x19d6bfb78c0>, 'Connection to coastwatch.pfeg.noaa.gov timed out. (connect timeout=15)'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "‚ùå N√£o foi poss√≠vel baixar dados de SST de nenhuma fonte.\nPoss√≠veis solu√ß√µes:\n  1. Verificar conex√£o de internet\n  2. Tentar novamente mais tarde (servidores NOAA podem estar sobrecarregados)\n  3. Usar dados SST pr√©-baixados (coloque em data/raw_sst.nc)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     79\u001b[39m     sst_df = fetch_sst_erddap_short_timeout(cfg.region, \u001b[33m\"\u001b[39m\u001b[33m2010-01-01\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2023-12-31\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sst_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sst_df) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     83\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m‚ùå N√£o foi poss√≠vel baixar dados de SST de nenhuma fonte.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPoss√≠veis solu√ß√µes:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m  1. Verificar conex√£o de internet\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m  2. Tentar novamente mais tarde (servidores NOAA podem estar sobrecarregados)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     87\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m  3. Usar dados SST pr√©-baixados (coloque em data/raw_sst.nc)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m     )\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ SST carregada: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sst_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m registros\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m sst_df.head()\n",
      "\u001b[31mRuntimeError\u001b[39m: ‚ùå N√£o foi poss√≠vel baixar dados de SST de nenhuma fonte.\nPoss√≠veis solu√ß√µes:\n  1. Verificar conex√£o de internet\n  2. Tentar novamente mais tarde (servidores NOAA podem estar sobrecarregados)\n  3. Usar dados SST pr√©-baixados (coloque em data/raw_sst.nc)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "def fetch_sst_noaa_opendap(\n",
    "    region: RegionBBox,\n",
    "    start_date: str = \"2010-01-01\",\n",
    "    end_date: str = \"2023-12-31\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Baixa SST mensal do NOAA OISST via OPeNDAP (xarray).\n",
    "    Mais confi√°vel que CSV via ERDDAP.\n",
    "    \"\"\"\n",
    "    print(f\"Baixando SST via OPeNDAP (xarray)...\")\n",
    "    \n",
    "    try:\n",
    "        # URL OPeNDAP para OISST v2.1 monthly\n",
    "        url = \"https://www.ncei.noaa.gov/thredds/dodsC/OisstBase/NetCDF/V2.1/AVHRR/monthly/sst.mnmean.nc\"\n",
    "        \n",
    "        # Abrir com xarray\n",
    "        ds = xr.open_dataset(url, decode_times=True)\n",
    "        \n",
    "        # Selecionar regi√£o e per√≠odo\n",
    "        ds_subset = ds.sel(\n",
    "            lat=slice(region.lat_min, region.lat_max),\n",
    "            lon=slice(region.lon_min + 360, region.lon_max + 360),  # lon em [0, 360]\n",
    "            time=slice(start_date, end_date)\n",
    "        )\n",
    "        \n",
    "        # Converter para DataFrame\n",
    "        df = ds_subset['sst'].to_dataframe().reset_index()\n",
    "        df = df.dropna(subset=['sst'])\n",
    "        \n",
    "        # Ajustar longitude para [-180, 180]\n",
    "        df['lon'] = df['lon'].apply(lambda x: x - 360 if x > 180 else x)\n",
    "        df = df.rename(columns={'lat': 'latitude', 'lon': 'longitude'})\n",
    "        \n",
    "        print(f\"SST: {len(df)} pontos, {df['time'].min()} a {df['time'].max()}\")\n",
    "        return df[['time', 'latitude', 'longitude', 'sst']]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro OPeNDAP: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_sst_erddap_short_timeout(\n",
    "    region: RegionBBox,\n",
    "    start_date: str = \"2010-01-01\",\n",
    "    end_date: str = \"2023-12-31\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Tenta ERDDAP CSV com timeout curto (15s).\"\"\"\n",
    "    base_url = \"https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg_LonPM180.csv\"\n",
    "    query = (\n",
    "        f\"?sst[({start_date}T00:00:00Z):1:({end_date}T00:00:00Z)]\"\n",
    "        f\"[({region.lat_min}):1:({region.lat_max})]\"\n",
    "        f\"[({region.lon_min}):1:({region.lon_max})]\"\n",
    "    )\n",
    "    url = base_url + query\n",
    "    print(f\"Tentando ERDDAP CSV (timeout 15s)...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        df = pd.read_csv(StringIO(response.text), skiprows=[1])\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "        print(f\"SST: {len(df)} pontos\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ERDDAP: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Tentar m√∫ltiplas fontes (sem fallback sint√©tico!)\n",
    "print(\"=== Baixando SST (dados REAIS) ===\\n\")\n",
    "\n",
    "sst_df = fetch_sst_noaa_opendap(cfg.region, \"2010-01-01\", \"2023-12-31\")\n",
    "\n",
    "if sst_df is None or len(sst_df) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è Tentando fonte alternativa (ERDDAP)...\\n\")\n",
    "    sst_df = fetch_sst_erddap_short_timeout(cfg.region, \"2010-01-01\", \"2023-12-31\")\n",
    "\n",
    "if sst_df is None or len(sst_df) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"‚ùå N√£o foi poss√≠vel baixar dados de SST de nenhuma fonte.\\n\"\n",
    "        \"Poss√≠veis solu√ß√µes:\\n\"\n",
    "        \"  1. Verificar conex√£o de internet\\n\"\n",
    "        \"  2. Tentar novamente mais tarde (servidores NOAA podem estar sobrecarregados)\\n\"\n",
    "        \"  3. Usar dados SST pr√©-baixados (coloque em data/raw_sst.nc)\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ SST carregada: {len(sst_df)} registros\")\n",
    "sst_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sst_to_grid(sst_df: pd.DataFrame, grid: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega SST para c√©lulas do grid (m√©dia por c√©lula √ó m√™s).\n",
    "    \"\"\"\n",
    "    sst_df = sst_df.copy()\n",
    "    sst_df[\"year_month\"] = sst_df[\"time\"].dt.to_period(\"M\")\n",
    "    \n",
    "    # Assign each SST point to a grid cell\n",
    "    def assign_cell(row):\n",
    "        lat, lon = row[\"latitude\"], row[\"longitude\"]\n",
    "        for _, cell in grid.iterrows():\n",
    "            if (cell[\"lat_min\"] <= lat < cell[\"lat_max\"] and\n",
    "                cell[\"lon_min\"] <= lon < cell[\"lon_max\"]):\n",
    "                return f\"{cell['lat']:.1f}_{cell['lon']:.1f}\"\n",
    "        return None\n",
    "    \n",
    "    # Vectorized assignment (faster)\n",
    "    lat_idx = np.floor((sst_df[\"latitude\"] - cfg.region.lat_min) / cfg.grid_deg).astype(int)\n",
    "    lon_idx = np.floor((sst_df[\"longitude\"] - cfg.region.lon_min) / cfg.grid_deg).astype(int)\n",
    "    \n",
    "    cell_lat = cfg.region.lat_min + (lat_idx + 0.5) * cfg.grid_deg\n",
    "    cell_lon = cfg.region.lon_min + (lon_idx + 0.5) * cfg.grid_deg\n",
    "    sst_df[\"cell_id\"] = [f\"{la:.1f}_{lo:.1f}\" for la, lo in zip(cell_lat, cell_lon)]\n",
    "    \n",
    "    # Aggregate\n",
    "    agg = sst_df.groupby([\"cell_id\", \"year_month\"]).agg(\n",
    "        sst_mean=(\"sst\", \"mean\"),\n",
    "        sst_std=(\"sst\", \"std\"),\n",
    "        n_obs=(\"sst\", \"count\"),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Parse cell_id back to lat/lon\n",
    "    agg[[\"lat\", \"lon\"]] = agg[\"cell_id\"].str.split(\"_\", expand=True).astype(float)\n",
    "    agg[\"time\"] = agg[\"year_month\"].dt.to_timestamp()\n",
    "    \n",
    "    return agg[[\"time\", \"lat\", \"lon\", \"cell_id\", \"sst_mean\", \"sst_std\", \"n_obs\"]]\n",
    "\n",
    "\n",
    "sst_grid = aggregate_sst_to_grid(sst_df, grid)\n",
    "print(f\"SST agregada: {len(sst_grid)} registros (c√©lula √ó m√™s)\")\n",
    "sst_grid.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d79a02",
   "metadata": {},
   "source": [
    "## 3) Ocorr√™ncias OBIS ‚Äî tubar√µes e presas\n",
    "\n",
    "Usar `pyobis` para buscar ocorr√™ncias por esp√©cie/fam√≠lia na regi√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553535c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyobis import occurrences\n",
    "    HAS_PYOBIS = True\n",
    "except ImportError:\n",
    "    HAS_PYOBIS = False\n",
    "    print(\"pyobis n√£o instalado. Instale com: poetry install -E data\")\n",
    "\n",
    "\n",
    "def fetch_obis_occurrences(\n",
    "    taxon: str,\n",
    "    region: RegionBBox,\n",
    "    start_year: int = 2010,\n",
    "    end_year: int = 2023,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Busca ocorr√™ncias OBIS para um taxon na regi√£o.\n",
    "    \"\"\"\n",
    "    if not HAS_PYOBIS:\n",
    "        return _generate_synthetic_occurrences(taxon, region, start_year, end_year)\n",
    "    \n",
    "    try:\n",
    "        geometry = f\"POLYGON(({region.lon_min} {region.lat_min}, {region.lon_max} {region.lat_min}, {region.lon_max} {region.lat_max}, {region.lon_min} {region.lat_max}, {region.lon_min} {region.lat_min}))\"\n",
    "        \n",
    "        result = occurrences.search(\n",
    "            scientificname=taxon,\n",
    "            geometry=geometry,\n",
    "            startdate=f\"{start_year}-01-01\",\n",
    "            enddate=f\"{end_year}-12-31\",\n",
    "        )\n",
    "        \n",
    "        if result is None or len(result) == 0:\n",
    "            print(f\"  {taxon}: 0 ocorr√™ncias (usando sint√©tico)\")\n",
    "            return _generate_synthetic_occurrences(taxon, region, start_year, end_year)\n",
    "        \n",
    "        df = pd.DataFrame(result)\n",
    "        df[\"taxon\"] = taxon\n",
    "        print(f\"  {taxon}: {len(df)} ocorr√™ncias\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  {taxon}: erro ({e}), usando sint√©tico\")\n",
    "        return _generate_synthetic_occurrences(taxon, region, start_year, end_year)\n",
    "\n",
    "\n",
    "def _generate_synthetic_occurrences(\n",
    "    taxon: str,\n",
    "    region: RegionBBox,\n",
    "    start_year: int,\n",
    "    end_year: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Gera ocorr√™ncias sint√©ticas para testes.\"\"\"\n",
    "    np.random.seed(hash(taxon) % 2**32)\n",
    "    n = np.random.randint(50, 500)\n",
    "    \n",
    "    dates = pd.date_range(f\"{start_year}-01-01\", f\"{end_year}-12-31\", freq=\"D\")\n",
    "    sampled_dates = np.random.choice(dates, n)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"taxon\": taxon,\n",
    "        \"decimalLatitude\": np.random.uniform(region.lat_min, region.lat_max, n),\n",
    "        \"decimalLongitude\": np.random.uniform(region.lon_min, region.lon_max, n),\n",
    "        \"eventDate\": sampled_dates,\n",
    "        \"basisOfRecord\": \"synthetic\",\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"=== Buscando ocorr√™ncias OBIS ===\")\n",
    "print(\"\\nTubar√µes:\")\n",
    "shark_occ = pd.concat([\n",
    "    fetch_obis_occurrences(sp, cfg.region) for sp in tqdm(cfg.shark_species)\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"\\nPresas/guildas:\")\n",
    "prey_occ = pd.concat([\n",
    "    fetch_obis_occurrences(g, cfg.region) for g in tqdm(cfg.prey_groups)\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal tubar√µes: {len(shark_occ)} ocorr√™ncias\")\n",
    "print(f\"Total presas: {len(prey_occ)} ocorr√™ncias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_occurrences_to_grid(\n",
    "    occ_df: pd.DataFrame,\n",
    "    grid: pd.DataFrame,\n",
    "    region: RegionBBox,\n",
    "    grid_deg: float,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega ocorr√™ncias para c√©lulas do grid (contagem por c√©lula √ó m√™s √ó taxon).\n",
    "    \"\"\"\n",
    "    df = occ_df.copy()\n",
    "    \n",
    "    # Padronizar nomes de colunas\n",
    "    lat_col = \"decimalLatitude\" if \"decimalLatitude\" in df.columns else \"latitude\"\n",
    "    lon_col = \"decimalLongitude\" if \"decimalLongitude\" in df.columns else \"longitude\"\n",
    "    date_col = \"eventDate\" if \"eventDate\" in df.columns else \"date\"\n",
    "    \n",
    "    df[\"lat\"] = df[lat_col]\n",
    "    df[\"lon\"] = df[lon_col]\n",
    "    df[\"time\"] = pd.to_datetime(df[date_col])\n",
    "    df[\"year_month\"] = df[\"time\"].dt.to_period(\"M\")\n",
    "    \n",
    "    # Assign to grid cells\n",
    "    lat_idx = np.floor((df[\"lat\"] - region.lat_min) / grid_deg).astype(int)\n",
    "    lon_idx = np.floor((df[\"lon\"] - region.lon_min) / grid_deg).astype(int)\n",
    "    \n",
    "    cell_lat = region.lat_min + (lat_idx + 0.5) * grid_deg\n",
    "    cell_lon = region.lon_min + (lon_idx + 0.5) * grid_deg\n",
    "    df[\"cell_id\"] = [f\"{la:.1f}_{lo:.1f}\" for la, lo in zip(cell_lat, cell_lon)]\n",
    "    \n",
    "    # Aggregate\n",
    "    agg = df.groupby([\"taxon\", \"cell_id\", \"year_month\"]).agg(\n",
    "        count=(\"lat\", \"count\"),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Parse cell_id\n",
    "    agg[[\"lat\", \"lon\"]] = agg[\"cell_id\"].str.split(\"_\", expand=True).astype(float)\n",
    "    agg[\"time\"] = agg[\"year_month\"].dt.to_timestamp()\n",
    "    \n",
    "    return agg[[\"time\", \"lat\", \"lon\", \"cell_id\", \"taxon\", \"count\"]]\n",
    "\n",
    "\n",
    "shark_grid = aggregate_occurrences_to_grid(shark_occ, grid, cfg.region, cfg.grid_deg)\n",
    "prey_grid = aggregate_occurrences_to_grid(prey_occ, grid, cfg.region, cfg.grid_deg)\n",
    "\n",
    "print(f\"Tubar√µes agregados: {len(shark_grid)} registros (c√©lula √ó m√™s √ó esp√©cie)\")\n",
    "print(f\"Presas agregadas: {len(prey_grid)} registros (c√©lula √ó m√™s √ó grupo)\")\n",
    "\n",
    "shark_grid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37badf22",
   "metadata": {},
   "source": [
    "## 4) Normaliza√ß√£o ‚Äî proxy de abund√¢ncia\n",
    "\n",
    "Converter contagens brutas em √≠ndice de ocorr√™ncia normalizado (proxy de abund√¢ncia relativa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_occurrence_index(df: pd.DataFrame, count_col: str = \"count\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normaliza contagens para √≠ndice [0, 1] por taxon.\n",
    "    Usa min-max scaling dentro de cada taxon.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    def minmax_scale(x):\n",
    "        if x.max() == x.min():\n",
    "            return x * 0 + 0.5  # constant ‚Üí 0.5\n",
    "        return (x - x.min()) / (x.max() - x.min())\n",
    "    \n",
    "    df[\"occurrence_index\"] = df.groupby(\"taxon\")[count_col].transform(minmax_scale)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "shark_grid = normalize_occurrence_index(shark_grid)\n",
    "prey_grid = normalize_occurrence_index(prey_grid)\n",
    "\n",
    "print(\"Estat√≠sticas do √≠ndice de ocorr√™ncia (tubar√µes):\")\n",
    "print(shark_grid.groupby(\"taxon\")[\"occurrence_index\"].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d399d2f",
   "metadata": {},
   "source": [
    "## 5) Salvar datasets processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar em parquet (eficiente e preserva tipos)\n",
    "sst_grid.to_parquet(DATA_DIR / \"sst_grid.parquet\", index=False)\n",
    "shark_grid.to_parquet(DATA_DIR / \"shark_occurrences_grid.parquet\", index=False)\n",
    "prey_grid.to_parquet(DATA_DIR / \"prey_occurrences_grid.parquet\", index=False)\n",
    "\n",
    "print(\"Datasets salvos em data/:\")\n",
    "for f in DATA_DIR.glob(\"*.parquet\"):\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"  {f.name}: {size_kb:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d0db5",
   "metadata": {},
   "source": [
    "## 6) Visualiza√ß√£o r√°pida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    HAS_PLOT = True\n",
    "except ImportError:\n",
    "    HAS_PLOT = False\n",
    "    print(\"matplotlib/seaborn n√£o instalados. Instale com: poetry install -E plot\")\n",
    "\n",
    "if HAS_PLOT:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # SST m√©dia por c√©lula\n",
    "    sst_mean = sst_grid.groupby([\"lat\", \"lon\"])[\"sst_mean\"].mean().reset_index()\n",
    "    ax = axes[0]\n",
    "    scatter = ax.scatter(sst_mean[\"lon\"], sst_mean[\"lat\"], c=sst_mean[\"sst_mean\"], \n",
    "                         cmap=\"RdYlBu_r\", s=20)\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    ax.set_title(\"SST m√©dia (¬∞C)\")\n",
    "    plt.colorbar(scatter, ax=ax)\n",
    "    \n",
    "    # Ocorr√™ncias de tubar√µes\n",
    "    shark_total = shark_grid.groupby([\"lat\", \"lon\"])[\"count\"].sum().reset_index()\n",
    "    ax = axes[1]\n",
    "    scatter = ax.scatter(shark_total[\"lon\"], shark_total[\"lat\"], c=shark_total[\"count\"],\n",
    "                         cmap=\"Reds\", s=20)\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    ax.set_title(\"Ocorr√™ncias tubar√µes (total)\")\n",
    "    plt.colorbar(scatter, ax=ax)\n",
    "    \n",
    "    # S√©rie temporal SST\n",
    "    sst_ts = sst_grid.groupby(\"time\")[\"sst_mean\"].mean().reset_index()\n",
    "    ax = axes[2]\n",
    "    ax.plot(sst_ts[\"time\"], sst_ts[\"sst_mean\"])\n",
    "    ax.set_xlabel(\"Tempo\")\n",
    "    ax.set_ylabel(\"SST m√©dia (¬∞C)\")\n",
    "    ax.set_title(\"SST m√©dia regional\")\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(DATA_DIR / \"data_overview.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Figura salva em {DATA_DIR / 'data_overview.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4198032",
   "metadata": {},
   "source": [
    "## Pr√≥ximos passos\n",
    "\n",
    "Com os dados agregados em `data/`, podemos:\n",
    "\n",
    "1. **03_feature_engineering.ipynb** ‚Äî Criar features derivadas (ŒîSST, tend√™ncias, lags)\n",
    "2. **04_nssm_baseline.ipynb** ‚Äî Treinar modelo determin√≠stico (NSSM)\n",
    "3. **05_dmm_counterfactual.ipynb** ‚Äî Treinar DMM e gerar ensembles contrafactuais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nova-selachiia-oFZsEAWQ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
