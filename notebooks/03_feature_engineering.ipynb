{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349171f8",
   "metadata": {},
   "source": [
    "# 03. Feature Engineering for Spatiotemporal Modeling\n",
    "\n",
    "**Objective**: Transform heterogeneous raw data into a unified, model-ready feature matrix suitable for counterfactual analysis of shark population dynamics.\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "**Data transformation pipeline**:\n",
    "1. **Spatial aggregation**: Point observations â†’ 1Â°Ã—1Â° grid cells (reduces dimensionality by ~100Ã—)\n",
    "2. **Temporal aggregation**: Daily/irregular â†’ monthly time steps (T = 168 months for 2010-2023)\n",
    "3. **Feature derivation**: SST gradients, temporal lags, cyclical encodings\n",
    "4. **Missing data handling**: Quality flags + complete-case subsetting\n",
    "\n",
    "**Mathematical notation**:\n",
    "- Grid cell: $c_{ij} = (\\phi_i, \\lambda_j)$ where $i \\in \\{1,...,N_{lat}\\}$, $j \\in \\{1,...,N_{lon}\\}$\n",
    "- Time step: $t \\in \\{1,...,T\\}$ (monthly)\n",
    "- Response variable: $Y_{ijt} \\in \\{0, 1\\}$ (shark presence/absence)\n",
    "- Covariates: $\\mathbf{X}_{ijt} = (SST_{ijt}, E_{ijt}, P_{ijt}, ...)$\n",
    "\n",
    "**Memory budget**: Target < 4GB working memory (safe for 8GB RAM systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10b00b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete\n",
      "   Project root: c:\\Users\\rugge_p2gkz2r\\Desktop\\nova-research\\nova-selachiia-1\n",
      "   Output directory: c:\\Users\\rugge_p2gkz2r\\Desktop\\nova-research\\nova-selachiia-1\\data\\processed\n",
      "\n",
      "ðŸ“‹ Configuration:\n",
      "   Region: -35.0Â° to 5.0Â° N\n",
      "           -50.0Â° to -30.0Â° E\n",
      "   Grid resolution: 1.0Â°\n",
      "   Study period: 2010-2023\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Import libraries and configuration\n",
    "# ============================================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "ROOT_DIR = Path.cwd().parent\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project config classes\n",
    "from nova_selachiia.config import ProjectConfig, RegionBBox, as_tuple\n",
    "\n",
    "# Initialize configuration\n",
    "cfg = ProjectConfig(\n",
    "    region=RegionBBox(lat_min=-35.0, lat_max=5.0, lon_min=-50.0, lon_max=-30.0),\n",
    "    grid_deg=1.0,\n",
    "    shark_species=as_tuple([\n",
    "        \"Prionace glauca\",\n",
    "        \"Carcharhinus longimanus\",\n",
    "        \"Sphyrna lewini\",\n",
    "        \"Isurus oxyrinchus\",\n",
    "        \"Carcharhinus leucas\",\n",
    "        \"Galeocerdo cuvier\",\n",
    "    ]),\n",
    "    prey_groups=as_tuple([\n",
    "        \"Clupeidae\",\n",
    "        \"Engraulidae\",\n",
    "        \"Scombridae\",\n",
    "        \"Carangidae\",\n",
    "        \"Loliginidae\",\n",
    "        \"Octopodidae\",\n",
    "        \"Trichiuridae\",\n",
    "        \"Mugilidae\",\n",
    "    ]),\n",
    "    collapse_threshold_quantile=0.10,\n",
    ")\n",
    "\n",
    "# Study period (separate variables since ProjectConfig is frozen)\n",
    "START_YEAR = 2010\n",
    "END_YEAR = 2023\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"âœ… Setup complete\")\n",
    "print(f\"   Project root: {ROOT_DIR}\")\n",
    "print(f\"   Output directory: {PROCESSED_DIR}\")\n",
    "print(f\"\\nðŸ“‹ Configuration:\")\n",
    "print(f\"   Region: {cfg.region.lat_min}Â° to {cfg.region.lat_max}Â° N\")\n",
    "print(f\"           {cfg.region.lon_min}Â° to {cfg.region.lon_max}Â° E\")\n",
    "print(f\"   Grid resolution: {cfg.grid_deg}Â°\")\n",
    "print(f\"   Study period: {START_YEAR}-{END_YEAR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25fc926",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data\n",
    "\n",
    "Load the cleaned datasets from the EDA notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3aa7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from raw sources...\n",
      "\n",
      "âœ… Tiger shark         :   19 records (parquet)\n",
      "âœ… Bull shark          :   17 records (parquet)\n",
      "âœ… Oceanic whitetip    :    7 records (parquet)\n",
      "âœ… Carangidae          : 1,384 records (parquet)\n",
      "âœ… Clupeidae           :   26 records (parquet)\n",
      "âœ… Engraulidae         :   65 records (parquet)\n",
      "âœ… Mugilidae           :  110 records (parquet)\n",
      "âœ… Octopodidae         :    9 records (parquet)\n",
      "âœ… Scombridae          :   37 records (parquet)\n",
      "âœ… Trichiuridae        :    1 records (parquet)\n",
      "\n",
      "ðŸ“Š Total species loaded: 3 sharks, 7 prey families\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA: Re-run minimal loading from 02_data_analysis\n",
    "# ============================================================================\n",
    "print(\"Loading data from raw sources...\\n\")\n",
    "\n",
    "# Helper function for Parquet/CSV fallback with size comparison\n",
    "def read_file_with_fallback(parquet_path, csv_path):\n",
    "    \"\"\"Try Parquet first, fall back to CSV if error. Compare sizes.\"\"\"\n",
    "    parquet_df = None\n",
    "    csv_df = None\n",
    "    \n",
    "    # Try Parquet\n",
    "    try:\n",
    "        if parquet_path.exists():\n",
    "            parquet_df = pd.read_parquet(parquet_path)\n",
    "    except Exception as e:\n",
    "        print(f\"      âš ï¸  Parquet error ({e.__class__.__name__})\")\n",
    "    \n",
    "    # Try CSV\n",
    "    try:\n",
    "        if csv_path.exists():\n",
    "            csv_df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"      âš ï¸  CSV error ({e.__class__.__name__})\")\n",
    "    \n",
    "    # Compare and choose larger dataset\n",
    "    if parquet_df is not None and csv_df is not None:\n",
    "        if len(csv_df) > len(parquet_df):\n",
    "            print(f\"      âš ï¸  CSV has MORE data ({len(csv_df):,} vs {len(parquet_df):,}) - using CSV\")\n",
    "            return csv_df, \"csv\"\n",
    "        else:\n",
    "            return parquet_df, \"parquet\"\n",
    "    elif parquet_df is not None:\n",
    "        return parquet_df, \"parquet\"\n",
    "    elif csv_df is not None:\n",
    "        return csv_df, \"csv\"\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Neither Parquet nor CSV found\")\n",
    "\n",
    "# 1. SHARK DATA\n",
    "shark_species = [\n",
    "    (\"Galeocerdo cuvier\", \"Tiger shark\"),\n",
    "    (\"Carcharhinus leucas\", \"Bull shark\"),\n",
    "    (\"Carcharhinus longimanus\", \"Oceanic whitetip\")\n",
    "]\n",
    "\n",
    "shark_data = {}\n",
    "for scientific, common in shark_species:\n",
    "    genus, species = scientific.split()\n",
    "    base_name = f\"shark_{genus}_{species}\"\n",
    "    \n",
    "    parquet_path = RAW_DIR / \"raw_parquet\" / f\"{base_name}.parquet\"\n",
    "    csv_path = RAW_DIR / \"raw_csv\" / base_name / f\"{base_name}.csv\"\n",
    "    \n",
    "    if parquet_path.exists() or csv_path.exists():\n",
    "        df, fmt = read_file_with_fallback(parquet_path, csv_path)\n",
    "        shark_data[scientific] = df\n",
    "        print(f\"âœ… {common:20s}: {len(df):4,} records ({fmt})\")\n",
    "\n",
    "# 2. PREY DATA\n",
    "prey_families = [\n",
    "    \"Carangidae\", \"Clupeidae\", \"Engraulidae\", \"Mugilidae\",\n",
    "    \"Octopodidae\", \"Scombridae\", \"Trichiuridae\"\n",
    "]\n",
    "\n",
    "prey_data = {}\n",
    "for family in prey_families:\n",
    "    base_name = f\"prey_{family}\"\n",
    "    parquet_path = RAW_DIR / \"raw_parquet\" / f\"{base_name}.parquet\"\n",
    "    csv_path = RAW_DIR / \"raw_csv\" / f\"{base_name}.csv\"\n",
    "    \n",
    "    if parquet_path.exists() or csv_path.exists():\n",
    "        df, fmt = read_file_with_fallback(parquet_path, csv_path)\n",
    "        prey_data[family] = df\n",
    "        print(f\"âœ… {family:20s}: {len(df):4,} records ({fmt})\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total species loaded: {len(shark_data)} sharks, {len(prey_data)} prey families\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6732e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing occurrence data...\n",
      "\n",
      "âœ… Occurrences processed:\n",
      "   Sharks: 25 records\n",
      "   Prey:   716 records\n",
      "   Total:  741 records\n",
      "\n",
      "ðŸ“… Temporal coverage:\n",
      "   2010-2023\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROCESS OCCURRENCE DATA: Filter and standardize\n",
    "# ============================================================================\n",
    "print(\"Processing occurrence data...\\n\")\n",
    "\n",
    "def process_occurrences(data_dict, taxon_type):\n",
    "    \"\"\"Clean and filter OBIS occurrence data with detailed diagnostics.\"\"\"\n",
    "    all_occ = []\n",
    "    \n",
    "    for taxon, df in data_dict.items():\n",
    "        n_initial = len(df)\n",
    "        print(f\"\\n  {taxon} ({taxon_type}):\")\n",
    "        print(f\"    Initial records: {n_initial:,}\")\n",
    "        \n",
    "        # Check for required columns\n",
    "        if 'decimalLatitude' not in df.columns or 'decimalLongitude' not in df.columns:\n",
    "            print(f\"    âš ï¸ Missing coordinate columns! Available: {list(df.columns)}\")\n",
    "            continue\n",
    "        \n",
    "        # Show initial spatial extent\n",
    "        print(f\"    Initial spatial extent:\")\n",
    "        print(f\"      Lat: [{df['decimalLatitude'].min():.2f}, {df['decimalLatitude'].max():.2f}]\")\n",
    "        print(f\"      Lon: [{df['decimalLongitude'].min():.2f}, {df['decimalLongitude'].max():.2f}]\")\n",
    "        \n",
    "        # Filter to region and time period\n",
    "        df_clean = df[\n",
    "            (df['decimalLatitude'] >= cfg.region.lat_min) &\n",
    "            (df['decimalLatitude'] <= cfg.region.lat_max) &\n",
    "            (df['decimalLongitude'] >= cfg.region.lon_min) &\n",
    "            (df['decimalLongitude'] <= cfg.region.lon_max)\n",
    "        ].copy()\n",
    "        \n",
    "        n_after_spatial = len(df_clean)\n",
    "        print(f\"    After spatial filter [{cfg.region.lat_min}Â° to {cfg.region.lat_max}Â°N, {cfg.region.lon_min}Â° to {cfg.region.lon_max}Â°E]: {n_after_spatial:,} ({n_after_spatial/n_initial*100:.1f}%)\")\n",
    "        \n",
    "        if len(df_clean) == 0:\n",
    "            print(f\"    âš ï¸ All records removed by spatial filter!\")\n",
    "            continue\n",
    "        \n",
    "        # Parse dates\n",
    "        if 'eventDate' in df_clean.columns:\n",
    "            df_clean['date'] = pd.to_datetime(df_clean['eventDate'], errors='coerce')\n",
    "            n_with_dates = df_clean['date'].notna().sum()\n",
    "            print(f\"    Records with valid dates: {n_with_dates:,} ({n_with_dates/len(df_clean)*100:.1f}%)\")\n",
    "            \n",
    "            if n_with_dates > 0:\n",
    "                print(f\"    Date range: {df_clean['date'].min()} to {df_clean['date'].max()}\")\n",
    "            \n",
    "            df_clean = df_clean[\n",
    "                (df_clean['date'].dt.year >= START_YEAR) &\n",
    "                (df_clean['date'].dt.year <= END_YEAR)\n",
    "            ]\n",
    "            n_after_temporal = len(df_clean)\n",
    "            print(f\"    After temporal filter [{START_YEAR}-{END_YEAR}]: {n_after_temporal:,} ({n_after_temporal/n_after_spatial*100:.1f}%)\")\n",
    "            \n",
    "            df_clean['year'] = df_clean['date'].dt.year\n",
    "            df_clean['month'] = df_clean['date'].dt.month\n",
    "        else:\n",
    "            print(f\"    âš ï¸ No 'eventDate' column - assigning to {(START_YEAR + END_YEAR) // 2}\")\n",
    "            # No date info - assign to median year\n",
    "            df_clean['year'] = (START_YEAR + END_YEAR) // 2\n",
    "            df_clean['month'] = 6  # June (mid-year)\n",
    "            n_after_temporal = len(df_clean)\n",
    "        \n",
    "        if len(df_clean) == 0:\n",
    "            print(f\"    âš ï¸ All records removed by temporal filter!\")\n",
    "            continue\n",
    "        \n",
    "        # Standardize columns\n",
    "        df_clean = df_clean.rename(columns={\n",
    "            'decimalLatitude': 'lat',\n",
    "            'decimalLongitude': 'lon'\n",
    "        })\n",
    "        df_clean['taxon'] = taxon\n",
    "        df_clean['type'] = taxon_type\n",
    "        \n",
    "        print(f\"    âœ“ Final records: {len(df_clean):,}\")\n",
    "        all_occ.append(df_clean[['lat', 'lon', 'year', 'month', 'taxon', 'type']])\n",
    "    \n",
    "    return pd.concat(all_occ, ignore_index=True) if all_occ else pd.DataFrame()\n",
    "\n",
    "# Process both sharks and prey\n",
    "shark_occ = process_occurrences(shark_data, 'predator')\n",
    "prey_occ = process_occurrences(prey_data, 'prey')\n",
    "all_occ = pd.concat([shark_occ, prey_occ], ignore_index=True)\n",
    "\n",
    "print(f\"âœ… Occurrences processed:\")\n",
    "print(f\"   Sharks: {len(shark_occ):,} records\")\n",
    "print(f\"   Prey:   {len(prey_occ):,} records\")\n",
    "print(f\"   Total:  {len(all_occ):,} records\")\n",
    "print(f\"\\nðŸ“… Temporal coverage:\")\n",
    "print(f\"   {all_occ['year'].min()}-{all_occ['year'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f274d",
   "metadata": {},
   "source": [
    "## 2. Spatiotemporal Grid Construction\n",
    "\n",
    "Generate regular 1Â°Ã—1Â° spatial grid with monthly temporal resolution covering the study domain.\n",
    "\n",
    "**Grid specification**:\n",
    "- Spatial extent: $\\phi \\in [\\phi_{min}, \\phi_{max}]$, $\\lambda \\in [\\lambda_{min}, \\lambda_{max}]$\n",
    "- Spatial resolution: $\\Delta\\phi = \\Delta\\lambda = 1Â°$ (approximately 111 km at equator)\n",
    "- Temporal extent: January 2010 â€“ December 2023\n",
    "- Temporal resolution: Monthly (month-start timestamps)\n",
    "\n",
    "**Total grid cells**: $N = N_{lat} \\times N_{lon} \\times T$\n",
    "\n",
    "This creates a 3D tensor structure: `grid[lat, lon, time]` flattened to tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82b5b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating spatiotemporal grid...\n",
      "\n",
      "Grid created successfully:\n",
      "   Spatial: 41 lats Ã— 21 lons = 861 cells\n",
      "   Temporal: 168 months (2010-01-01 to 2023-12-01)\n",
      "   Total: 144,648 grid cells (N_lat Ã— N_lon Ã— T = 144,648)\n",
      "\n",
      "   Memory estimate: 22.1 MB (with 20 features)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CREATE SPATIOTEMPORAL GRID\n",
    "# =============================================================================\n",
    "print(\"Creating spatiotemporal grid...\\n\")\n",
    "\n",
    "# Spatial grid (1Â° resolution)\n",
    "lats = np.arange(cfg.region.lat_min, cfg.region.lat_max + cfg.grid_deg, cfg.grid_deg)\n",
    "lons = np.arange(cfg.region.lon_min, cfg.region.lon_max + cfg.grid_deg, cfg.grid_deg)\n",
    "\n",
    "# Temporal grid (monthly)\n",
    "start_date = pd.Timestamp(f\"{START_YEAR}-01-01\")\n",
    "end_date = pd.Timestamp(f\"{END_YEAR}-12-31\")\n",
    "dates = pd.date_range(start_date, end_date, freq='MS')  # Month start\n",
    "\n",
    "# Create full grid (all combinations of lat Ã— lon Ã— time)\n",
    "grid_cells = []\n",
    "for date in dates:\n",
    "    for lat in lats:\n",
    "        for lon in lons:\n",
    "            grid_cells.append({\n",
    "                'date': date,\n",
    "                'year': date.year,\n",
    "                'month': date.month,\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'cell_id': f\"{lat:.1f}_{lon:.1f}\"\n",
    "            })\n",
    "\n",
    "grid = pd.DataFrame(grid_cells)\n",
    "\n",
    "# Mathematical validation\n",
    "n_lat, n_lon, n_time = len(lats), len(lons), len(dates)\n",
    "n_expected = n_lat * n_lon * n_time\n",
    "\n",
    "assert len(grid) == n_expected, f\"Grid size mismatch: {len(grid)} â‰  {n_expected}\"\n",
    "\n",
    "print(f\"Grid created successfully:\")\n",
    "print(f\"   Spatial: {n_lat} lats Ã— {n_lon} lons = {n_lat * n_lon} cells\")\n",
    "print(f\"   Temporal: {n_time} months ({dates[0].date()} to {dates[-1].date()})\")\n",
    "print(f\"   Total: {len(grid):,} grid cells (N_lat Ã— N_lon Ã— T = {n_expected:,})\")\n",
    "print(f\"\\n   Memory estimate: {len(grid) * 8 * 20 / 1024**2:.1f} MB (with 20 features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba8e23",
   "metadata": {},
   "source": [
    "## 3. Occurrence Data Aggregation\n",
    "\n",
    "Map point occurrences to grid cells using floor discretization.\n",
    "\n",
    "**Aggregation method**:\n",
    "For each occurrence point $(lat_k, lon_k, t_k)$:\n",
    "1. Compute grid indices: $i = \\lfloor (lat_k - \\phi_{min}) / \\Delta\\phi \\rfloor$, $j = \\lfloor (lon_k - \\lambda_{min}) / \\Delta\\lambda \\rfloor$\n",
    "2. Assign to cell $(c_{ij}, t_k)$\n",
    "3. Count occurrences per cell-time: $n_{ijt} = \\sum_k \\mathbb{1}[(lat_k, lon_k, t_k) \\in c_{ijt}]$\n",
    "\n",
    "**Binary encoding**: $Y_{ijt} = \\mathbb{1}[n_{ijt} > 0]$ (presence/absence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fda9ac86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating occurrences to spatiotemporal grid...\n",
      "\n",
      "Occurrences aggregated:\n",
      "   Shark cells with data: 18 / 144,648 (0.012%)\n",
      "   Prey cells with data:  112 / 144,648 (0.08%)\n",
      "\n",
      "Shark occurrence statistics:\n",
      "   Mean count per cell: 0.0002\n",
      "   Max count per cell: 3\n",
      "   Sparsity: 99.99% zeros (highly imbalanced)\n",
      "\n",
      "âœ“ Aggregation validation passed: total occurrences preserved\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AGGREGATE OCCURRENCES TO GRID\n",
    "# =============================================================================\n",
    "print(\"Aggregating occurrences to spatiotemporal grid...\\n\")\n",
    "\n",
    "def aggregate_to_grid(occ_df, grid_df):\n",
    "    \"\"\"\n",
    "    Aggregate occurrence points to spatiotemporal grid cells.\n",
    "    \n",
    "    Uses floor discretization: cell_i = floor((coord - min) / resolution)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    occ_df : pd.DataFrame\n",
    "        Occurrence data with columns: lat, lon, year, month\n",
    "    grid_df : pd.DataFrame\n",
    "        Full grid with columns: date, year, month, lat, lon, cell_id\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Grid with count and presence columns added\n",
    "    \"\"\"\n",
    "    if len(occ_df) == 0:\n",
    "        result = grid_df.copy()\n",
    "        result['count'] = 0\n",
    "        return result\n",
    "    \n",
    "    # Assign each occurrence to grid cell using floor discretization\n",
    "    occ_df = occ_df.copy()\n",
    "    occ_df['grid_lat'] = (np.floor(occ_df['lat'] / cfg.grid_deg) * cfg.grid_deg)\n",
    "    occ_df['grid_lon'] = (np.floor(occ_df['lon'] / cfg.grid_deg) * cfg.grid_deg)\n",
    "    \n",
    "    # Count occurrences per cell-time: n_ijt = Î£ 1[(lat_k, lon_k, t_k) âˆˆ c_ijt]\n",
    "    counts = occ_df.groupby(['year', 'month', 'grid_lat', 'grid_lon']).size().reset_index(name='count')\n",
    "    \n",
    "    # Merge with full grid (left join preserves all grid cells)\n",
    "    result = grid_df.merge(\n",
    "        counts,\n",
    "        left_on=['year', 'month', 'lat', 'lon'],\n",
    "        right_on=['year', 'month', 'grid_lat', 'grid_lon'],\n",
    "        how='left'\n",
    "    )\n",
    "    result['count'] = result['count'].fillna(0).astype(int)\n",
    "    \n",
    "    return result[['date', 'year', 'month', 'lat', 'lon', 'cell_id', 'count']]\n",
    "\n",
    "# Aggregate sharks and prey separately\n",
    "shark_grid = aggregate_to_grid(shark_occ, grid)\n",
    "prey_grid = aggregate_to_grid(prey_occ, grid)\n",
    "\n",
    "# Binary encoding: Y_ijt = 1[n_ijt > 0]\n",
    "shark_grid['presence'] = (shark_grid['count'] > 0).astype(int)\n",
    "prey_grid['presence'] = (prey_grid['count'] > 0).astype(int)\n",
    "\n",
    "# Validation: check sparsity\n",
    "shark_occupied = (shark_grid['count'] > 0).sum()\n",
    "prey_occupied = (prey_grid['count'] > 0).sum()\n",
    "shark_sparsity = 1 - shark_occupied / len(shark_grid)\n",
    "prey_sparsity = 1 - prey_occupied / len(prey_grid)\n",
    "\n",
    "print(f\"Occurrences aggregated:\")\n",
    "print(f\"   Shark cells with data: {shark_occupied:,} / {len(shark_grid):,} ({100 - shark_sparsity*100:.3f}%)\")\n",
    "print(f\"   Prey cells with data:  {prey_occupied:,} / {len(prey_grid):,} ({100 - prey_sparsity*100:.2f}%)\")\n",
    "print(f\"\\nShark occurrence statistics:\")\n",
    "print(f\"   Mean count per cell: {shark_grid['count'].mean():.4f}\")\n",
    "print(f\"   Max count per cell: {shark_grid['count'].max()}\")\n",
    "print(f\"   Sparsity: {shark_sparsity*100:.2f}% zeros (highly imbalanced)\")\n",
    "\n",
    "# VALIDATION: Confirm total occurrences are preserved\n",
    "assert shark_grid['count'].sum() == len(shark_occ), \"Shark occurrence count mismatch after aggregation\"\n",
    "assert prey_grid['count'].sum() == len(prey_occ), \"Prey occurrence count mismatch after aggregation\"\n",
    "print(f\"\\nâœ“ Aggregation validation passed: total occurrences preserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8a247",
   "metadata": {},
   "source": [
    "## 4. Environmental and Anthropogenic Driver Integration\n",
    "\n",
    "Integrate Sea Surface Temperature (SST) and fishing effort data into the spatiotemporal grid.\n",
    "\n",
    "**Interpolation Method**: Nearest-neighbor (NN) using KD-Tree for computational efficiency  \n",
    "$$\\text{SST}_{ijt} = \\text{SST}(\\text{argmin}_k \\| (lat_{ijt}, lon_{ijt}) - (lat_k, lon_k) \\|_2)$$\n",
    "\n",
    "**Fishing Effort Aggregation**: Sum total effort hours per cell-month  \n",
    "$$E_{ijt} = \\sum_{k \\in c_{ijt}} h_k \\quad \\text{(apparent fishing hours)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327b6f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST data from NOAA OISST v2.1...\n",
      "\n",
      "âœ“ SST data loaded successfully: 24,696 observations\n",
      "   Temperature range: 15.8Â°C to 29.7Â°C\n",
      "   Spatial resolution: ~2.0Â°\n",
      "   Temporal coverage: 2010-2023\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD SST DATA (MEMORY-EFFICIENT)\n",
    "# =============================================================================\n",
    "# Source: NOAA OISST v2.1 (Optimum Interpolation Sea Surface Temperature)\n",
    "# Native resolution: 1.0Â° Ã— 1.0Â°, monthly temporal aggregation\n",
    "# Units: degrees Celsius (Â°C)\n",
    "# =============================================================================\n",
    "print(\"Loading SST data from NOAA OISST v2.1...\\n\")\n",
    "\n",
    "ds_sst = xr.open_dataset(RAW_DIR / \"sst.mnmean.nc\")\n",
    "\n",
    "# SPATIAL FILTER: Apply boolean masks for study region\n",
    "# Note: OISST uses 0-360 longitude convention, requires adjustment\n",
    "lat_mask = (ds_sst.lat >= cfg.region.lat_min) & (ds_sst.lat <= cfg.region.lat_max)\n",
    "lon_mask = (ds_sst.lon >= cfg.region.lon_min + 360) & (ds_sst.lon <= cfg.region.lon_max + 360)\n",
    "\n",
    "sst_subset = ds_sst.where(lat_mask & lon_mask, drop=True)\n",
    "\n",
    "# Convert xarray â†’ pandas for efficient merging\n",
    "sst_df = sst_subset['sst'].to_dataframe().reset_index()\n",
    "sst_df['lon'] = sst_df['lon'] - 360  # Convert to [-180, 180] convention\n",
    "sst_df = sst_df.dropna(subset=['sst'])\n",
    "\n",
    "# TEMPORAL FILTER: Restrict to study period [start_year, end_year]\n",
    "sst_df['time'] = pd.to_datetime(sst_df['time'])\n",
    "sst_df = sst_df[\n",
    "    (sst_df['time'].dt.year >= START_YEAR) &\n",
    "    (sst_df['time'].dt.year <= END_YEAR)\n",
    "]\n",
    "sst_df['year'] = sst_df['time'].dt.year\n",
    "sst_df['month'] = sst_df['time'].dt.month\n",
    "\n",
    "# VALIDATION: Physical bounds for SST (0Â°C < SST < 40Â°C for tropical/subtropical)\n",
    "sst_min, sst_max = sst_df['sst'].min(), sst_df['sst'].max()\n",
    "assert 0 < sst_min and sst_max < 40, f\"SST out of physical bounds: [{sst_min:.1f}, {sst_max:.1f}]Â°C\"\n",
    "\n",
    "print(f\"âœ“ SST data loaded successfully: {len(sst_df):,} observations\")\n",
    "print(f\"   Temperature range: {sst_min:.1f}Â°C to {sst_max:.1f}Â°C\")\n",
    "print(f\"   Spatial resolution: ~{abs(sst_df['lat'].unique()[1] - sst_df['lat'].unique()[0]):.1f}Â°\")\n",
    "print(f\"   Temporal coverage: {sst_df['year'].min()}-{sst_df['year'].max()}\")\n",
    "\n",
    "ds_sst.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a6ee83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating SST to analysis grid via nearest-neighbor...\n",
      "\n",
      "Building KD-Tree and querying (expected runtime: 1-2 min)...\n",
      "\n",
      "âš ï¸  Data quality warning: 29,568 cells (20.44%) have SST distance > 3.0Â°\n",
      "   These cells will be flagged as missing SST (likely edge effects or temporal gaps)\n",
      "\n",
      "âœ“ SST interpolation complete:\n",
      "   Coverage (after quality control): 79.6% of grid cells\n",
      "   Mean distance to observation: 1.02Â°\n",
      "   Max distance (valid cells): 3.00Â°\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# INTERPOLATE SST TO GRID (Nearest-Neighbor via KD-Tree)\n",
    "# =============================================================================\n",
    "# Algorithm: For each grid cell (i,j) at time t, find the closest SST observation\n",
    "# using Euclidean distance in (lat, lon) space. KD-Tree provides O(log n) queries.\n",
    "# =============================================================================\n",
    "print(\"Interpolating SST to analysis grid via nearest-neighbor...\\n\")\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def interpolate_sst_to_grid(sst_df, grid_df):\n",
    "    \"\"\"\n",
    "    Interpolate SST observations to grid cells using nearest-neighbor.\n",
    "    \n",
    "    For each grid cell c_ij at time t:\n",
    "        SST_ijt = SST_k where k = argmin ||pos(c_ij) - pos(obs_k)||_2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sst_df : pd.DataFrame\n",
    "        SST observations with columns: lat, lon, sst, year, month\n",
    "    grid_df : pd.DataFrame\n",
    "        Target grid with columns: lat, lon, year, month\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Grid with interpolated SST values and distance to nearest observation\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    # Process each timestep independently (temporal alignment)\n",
    "    for (year, month), grid_subset in grid_df.groupby(['year', 'month']):\n",
    "        sst_subset = sst_df[(sst_df['year'] == year) & (sst_df['month'] == month)]\n",
    "        \n",
    "        if len(sst_subset) == 0:\n",
    "            # No SST data for this timestep â†’ assign NaN\n",
    "            grid_subset = grid_subset.copy()\n",
    "            grid_subset['sst'] = np.nan\n",
    "            grid_subset['sst_dist'] = np.nan\n",
    "            result.append(grid_subset)\n",
    "            continue\n",
    "        \n",
    "        # Build spatial index (KD-Tree) for O(log n) nearest-neighbor queries\n",
    "        sst_coords = sst_subset[['lat', 'lon']].values\n",
    "        grid_coords = grid_subset[['lat', 'lon']].values\n",
    "        \n",
    "        tree = cKDTree(sst_coords)\n",
    "        distances, indices = tree.query(grid_coords, k=1)\n",
    "        \n",
    "        # Assign interpolated values\n",
    "        grid_subset = grid_subset.copy()\n",
    "        grid_subset['sst'] = sst_subset.iloc[indices]['sst'].values\n",
    "        grid_subset['sst_dist'] = distances  # Euclidean distance (degrees)\n",
    "        \n",
    "        result.append(grid_subset)\n",
    "    \n",
    "    return pd.concat(result, ignore_index=True)\n",
    "\n",
    "# Execute interpolation\n",
    "print(\"Building KD-Tree and querying (expected runtime: 1-2 min)...\")\n",
    "grid_with_sst = interpolate_sst_to_grid(sst_df, grid)\n",
    "\n",
    "# VALIDATION: Check interpolation quality\n",
    "mean_dist = grid_with_sst['sst_dist'].mean()\n",
    "max_dist = grid_with_sst['sst_dist'].max()\n",
    "sst_coverage_initial = grid_with_sst['sst'].notna().sum() / len(grid_with_sst) * 100\n",
    "\n",
    "# Quality control: Set SST to NaN for cells with distance > 3Â° (unreliable extrapolation)\n",
    "MAX_ACCEPTABLE_DIST = 3.0\n",
    "n_far_cells = (grid_with_sst['sst_dist'] > MAX_ACCEPTABLE_DIST).sum()\n",
    "if n_far_cells > 0:\n",
    "    print(f\"\\nâš ï¸  Data quality warning: {n_far_cells:,} cells ({n_far_cells/len(grid_with_sst)*100:.2f}%) have SST distance > {MAX_ACCEPTABLE_DIST}Â°\")\n",
    "    print(f\"   These cells will be flagged as missing SST (likely edge effects or temporal gaps)\")\n",
    "    grid_with_sst.loc[grid_with_sst['sst_dist'] > MAX_ACCEPTABLE_DIST, 'sst'] = np.nan\n",
    "\n",
    "# Recalculate coverage after quality filtering\n",
    "sst_coverage = grid_with_sst['sst'].notna().sum() / len(grid_with_sst) * 100\n",
    "mean_dist_valid = grid_with_sst[grid_with_sst['sst'].notna()]['sst_dist'].mean()\n",
    "\n",
    "print(f\"\\nâœ“ SST interpolation complete:\")\n",
    "print(f\"   Coverage (after quality control): {sst_coverage:.1f}% of grid cells\")\n",
    "print(f\"   Mean distance to observation: {mean_dist_valid:.2f}Â°\")\n",
    "print(f\"   Max distance (valid cells): {grid_with_sst[grid_with_sst['sst'].notna()]['sst_dist'].max():.2f}Â°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a6dde62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fishing effort data from Global Fishing Watch...\n",
      "\n",
      "Located 5 annual directories\n",
      "âš  Row limit reached (1,000,000) - truncating for memory safety\n",
      "\n",
      "âœ“ Fishing data loaded: 1,897,339 observations\n",
      "   Available columns: ['date', 'year', 'month', 'cell_ll_lat', 'cell_ll_lon', 'flag', 'geartype', 'hours', 'fishing_hours', 'mmsi_present']\n",
      "   Effort metric: fishing_hours\n",
      "   Coordinates mapped: {'cell_ll_lat': 'lat', 'cell_ll_lon': 'lon', 'fishing_hours': 'effort'}\n",
      "   Total effort: 6,345,361 apparent fishing hours\n",
      "   Temporal span: 2020-2020\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD AND AGGREGATE FISHING EFFORT DATA\n",
    "# =============================================================================\n",
    "# Source: Global Fishing Watch (GFW) - fleet-daily aggregated to monthly\n",
    "# Metric: Apparent Fishing Hours (AIS-derived estimate)\n",
    "# Memory constraint: Spatial filtering applied immediately after read\n",
    "# =============================================================================\n",
    "print(\"Loading fishing effort data from Global Fishing Watch...\\n\")\n",
    "\n",
    "import re\n",
    "\n",
    "# Locate fishing data directories (Parquet preferred over CSV)\n",
    "fleet_dirs = []\n",
    "for pattern in ['fleet-monthly-parquet-*', 'fleet-monthly-csvs-*']:\n",
    "    fleet_dirs.extend(sorted((RAW_DIR / \"raw_parquet\").glob(pattern)))\n",
    "    fleet_dirs.extend(sorted((RAW_DIR / \"raw_csv\").glob(pattern)))\n",
    "fleet_dirs = sorted(set(fleet_dirs))[:5]  # Limit to 5 years for memory\n",
    "\n",
    "if len(fleet_dirs) == 0:\n",
    "    print(\"âš  No fishing data found - fishing features will be set to zero\")\n",
    "    fishing_df = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"Located {len(fleet_dirs)} annual directories\")\n",
    "    \n",
    "    # Spatial bounds with buffer for edge effects\n",
    "    lat_min, lat_max = cfg.region.lat_min - 5, cfg.region.lat_max + 5\n",
    "    lon_min, lon_max = cfg.region.lon_min - 5, cfg.region.lon_max + 5\n",
    "    \n",
    "    # MEMORY MANAGEMENT: Load with immediate spatial filtering\n",
    "    fishing_data = []\n",
    "    total_rows = 0\n",
    "    MAX_ROWS = 1_000_000  # Hard limit: ~50MB in memory\n",
    "    \n",
    "    for year_dir in fleet_dirs:\n",
    "        year = year_dir.name.split('-')[-1]\n",
    "        \n",
    "        # Prefer Parquet (columnar, compressed) over CSV\n",
    "        files = list(year_dir.glob(\"*.parquet\")) or list(year_dir.glob(\"*.csv\"))\n",
    "        file_type = \"parquet\" if files and files[0].suffix == \".parquet\" else \"csv\"\n",
    "        \n",
    "        for data_file in files:\n",
    "            if total_rows >= MAX_ROWS:\n",
    "                print(f\"âš  Row limit reached ({MAX_ROWS:,}) - truncating for memory safety\")\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Load and immediately filter (CRITICAL for RAM)\n",
    "                if file_type == \"parquet\":\n",
    "                    df = pd.read_parquet(data_file)\n",
    "                else:\n",
    "                    df = pd.read_csv(data_file, low_memory=False)\n",
    "                \n",
    "                # Apply spatial filter BEFORE any processing\n",
    "                if 'Lat' in df.columns and 'Lon' in df.columns:\n",
    "                    mask = ((df['Lat'] >= lat_min) & (df['Lat'] <= lat_max) &\n",
    "                           (df['Lon'] >= lon_min) & (df['Lon'] <= lon_max))\n",
    "                    df = df[mask]\n",
    "                \n",
    "                if len(df) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Extract temporal information from filename\n",
    "                date_match = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', data_file.stem)\n",
    "                if date_match:\n",
    "                    df['year'] = int(date_match.group(1))\n",
    "                    df['month'] = int(date_match.group(2))\n",
    "                    fishing_data.append(df)\n",
    "                    total_rows += len(df)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"âš  Failed to load {data_file.name}: {e}\")\n",
    "        \n",
    "        if total_rows >= MAX_ROWS:\n",
    "            break\n",
    "    \n",
    "    if fishing_data:\n",
    "        fishing_df = pd.concat(fishing_data, ignore_index=True)\n",
    "        print(f\"\\nâœ“ Fishing data loaded: {len(fishing_df):,} observations\")\n",
    "        print(f\"   Available columns: {list(fishing_df.columns)}\")\n",
    "        \n",
    "        # Standardize coordinate column names (case-insensitive search)\n",
    "        coord_mapping = {}\n",
    "        for col in fishing_df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if col_lower in ['lat', 'latitude', 'cell_ll_lat']:\n",
    "                coord_mapping[col] = 'lat'\n",
    "            elif col_lower in ['lon', 'longitude', 'long', 'cell_ll_lon']:\n",
    "                coord_mapping[col] = 'lon'\n",
    "        \n",
    "        # Standardize effort column name\n",
    "        effort_col = None\n",
    "        for col in ['Apparent_Fishing_Hours', 'Fishing_hours', 'fishing_hours', 'hours', 'effort']:\n",
    "            if col in fishing_df.columns:\n",
    "                effort_col = col\n",
    "                break\n",
    "        \n",
    "        if effort_col and len(coord_mapping) >= 2:\n",
    "            coord_mapping[effort_col] = 'effort'\n",
    "            fishing_df = fishing_df.rename(columns=coord_mapping)\n",
    "            \n",
    "            # VALIDATION: Effort must be non-negative\n",
    "            assert (fishing_df['effort'] >= 0).all(), \"Negative fishing effort detected\"\n",
    "            \n",
    "            print(f\"   Effort metric: {effort_col}\")\n",
    "            print(f\"   Coordinates mapped: {coord_mapping}\")\n",
    "            print(f\"   Total effort: {fishing_df['effort'].sum():,.0f} apparent fishing hours\")\n",
    "            print(f\"   Temporal span: {fishing_df['year'].min()}-{fishing_df['year'].max()}\")\n",
    "        else:\n",
    "            missing = []\n",
    "            if not effort_col:\n",
    "                missing.append(\"effort column\")\n",
    "            if len(coord_mapping) < 2:\n",
    "                missing.append(\"lat/lon columns\")\n",
    "            print(f\"âš  Required columns not identified: {', '.join(missing)}\")\n",
    "            fishing_df = pd.DataFrame()\n",
    "    else:\n",
    "        fishing_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c1be8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating fishing effort to spatiotemporal grid...\n",
      "\n",
      "âœ“ Fishing effort aggregated:\n",
      "   Cells with fishing activity: 413 / 144,648 (0.3%)\n",
      "   Mean effort (where >0): 213.0 hours/cell-month\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AGGREGATE FISHING EFFORT TO SPATIOTEMPORAL GRID\n",
    "# =============================================================================\n",
    "# Aggregation: E_ijt = Î£_{k âˆˆ c_ijt} h_k (sum of fishing hours per cell-month)\n",
    "# =============================================================================\n",
    "if len(fishing_df) > 0:\n",
    "    print(\"Aggregating fishing effort to spatiotemporal grid...\\n\")\n",
    "    \n",
    "    # Discretize coordinates to grid cells using floor function\n",
    "    fishing_df['grid_lat'] = np.floor(fishing_df['lat'] / cfg.grid_deg) * cfg.grid_deg\n",
    "    fishing_df['grid_lon'] = np.floor(fishing_df['lon'] / cfg.grid_deg) * cfg.grid_deg\n",
    "    \n",
    "    # Aggregate: sum effort, count unique vessels (approximation)\n",
    "    fishing_agg = fishing_df.groupby(['year', 'month', 'grid_lat', 'grid_lon']).agg({\n",
    "        'effort': ['sum', 'count', 'mean']\n",
    "    }).reset_index()\n",
    "    fishing_agg.columns = ['year', 'month', 'lat', 'lon', 'effort_hours', 'n_vessels', 'effort_per_vessel']\n",
    "    \n",
    "    # Merge with environmental grid (left join preserves all cells)\n",
    "    grid_with_sst = grid_with_sst.merge(\n",
    "        fishing_agg,\n",
    "        on=['year', 'month', 'lat', 'lon'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing values: no fishing = 0 effort\n",
    "    grid_with_sst['effort_hours'] = grid_with_sst['effort_hours'].fillna(0)\n",
    "    grid_with_sst['n_vessels'] = grid_with_sst['n_vessels'].fillna(0)\n",
    "    \n",
    "    # VALIDATION: Non-negative effort\n",
    "    assert (grid_with_sst['effort_hours'] >= 0).all(), \"Negative effort values after aggregation\"\n",
    "    \n",
    "    fishing_coverage = (grid_with_sst['effort_hours'] > 0).sum() / len(grid_with_sst) * 100\n",
    "    print(f\"âœ“ Fishing effort aggregated:\")\n",
    "    print(f\"   Cells with fishing activity: {(grid_with_sst['effort_hours'] > 0).sum():,} / {len(grid_with_sst):,} ({fishing_coverage:.1f}%)\")\n",
    "    print(f\"   Mean effort (where >0): {grid_with_sst[grid_with_sst['effort_hours'] > 0]['effort_hours'].mean():.1f} hours/cell-month\")\n",
    "else:\n",
    "    print(\"âš  No fishing data available - setting effort features to zero\")\n",
    "    grid_with_sst['effort_hours'] = 0\n",
    "    grid_with_sst['n_vessels'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba01921",
   "metadata": {},
   "source": [
    "## 5. Derived Feature Engineering\n",
    "\n",
    "Construct derived features from raw environmental and anthropogenic drivers:\n",
    "\n",
    "| Feature | Definition | Interpretation |\n",
    "|---------|-----------|----------------|\n",
    "| SST anomaly | $\\text{SST}_{anom} = \\text{SST}_{ijt} - \\overline{\\text{SST}}_{ij\\cdot}$ | Deviation from local monthly climatology |\n",
    "| Temporal lag | $\\text{SST}_{lag-k} = \\text{SST}_{ij,t-k}$ | Memory effects (habitat selection delay) |\n",
    "| Rate of change | $\\Delta\\text{SST} = \\text{SST}_{ijt} - \\text{SST}_{ij,t-1}$ | Environmental variability signal |\n",
    "| Latitudinal gradient | $\\nabla_\\phi \\text{SST} = \\partial\\text{SST}/\\partial\\phi$ | Thermal front detection |\n",
    "| Seasonality encoding | $\\sin(2\\pi m/12), \\cos(2\\pi m/12)$ | Cyclic seasonal pattern (Fourier basis) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65d0e1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all data sources into unified analysis dataset...\n",
      "\n",
      "âœ“ Data sources merged successfully\n",
      "   Total spatiotemporal cells: 144,648\n",
      "   Features: ['date', 'year', 'month', 'lat', 'lon', 'cell_id', 'sst', 'sst_dist', 'effort_hours', 'n_vessels', 'effort_per_vessel', 'shark_count', 'shark_presence', 'prey_count', 'prey_presence']\n",
      "\n",
      "   Data completeness summary:\n",
      "   â€¢ SST coverage: 115,080 cells (79.6%)\n",
      "   â€¢ Shark presence: 18 cells (0.012%)\n",
      "   â€¢ Prey presence: 112 cells (0.08%)\n",
      "   â€¢ Fishing activity: 413 cells (0.29%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MERGE ALL DATA SOURCES INTO UNIFIED ANALYSIS DATASET\n",
    "# =============================================================================\n",
    "# Schema: Each row represents a spatiotemporal cell c_ijt\n",
    "# Columns: coordinates, environmental drivers, anthropogenic pressure, response\n",
    "# =============================================================================\n",
    "print(\"Merging all data sources into unified analysis dataset...\\n\")\n",
    "\n",
    "# Initialize with environmental data\n",
    "final_df = grid_with_sst.copy()\n",
    "\n",
    "# Add shark occurrence data (response variable)\n",
    "final_df = final_df.merge(\n",
    "    shark_grid[['date', 'lat', 'lon', 'count', 'presence']],\n",
    "    on=['date', 'lat', 'lon'],\n",
    "    how='left',\n",
    "    suffixes=('', '_shark')\n",
    ")\n",
    "final_df = final_df.rename(columns={'count': 'shark_count', 'presence': 'shark_presence'})\n",
    "\n",
    "# Add prey occurrence data (biotic covariate)\n",
    "final_df = final_df.merge(\n",
    "    prey_grid[['date', 'lat', 'lon', 'count', 'presence']],\n",
    "    on=['date', 'lat', 'lon'],\n",
    "    how='left',\n",
    "    suffixes=('', '_prey')\n",
    ")\n",
    "final_df = final_df.rename(columns={'count': 'prey_count', 'presence': 'prey_presence'})\n",
    "\n",
    "# Handle missing values: absence = 0 occurrences\n",
    "final_df['shark_count'] = final_df['shark_count'].fillna(0).astype(int)\n",
    "final_df['shark_presence'] = final_df['shark_presence'].fillna(0).astype(int)\n",
    "final_df['prey_count'] = final_df['prey_count'].fillna(0).astype(int)\n",
    "final_df['prey_presence'] = final_df['prey_presence'].fillna(0).astype(int)\n",
    "\n",
    "# VALIDATION: Row count should match original grid size\n",
    "assert len(final_df) == len(grid), f\"Row count mismatch: {len(final_df)} vs {len(grid)}\"\n",
    "\n",
    "print(f\"âœ“ Data sources merged successfully\")\n",
    "print(f\"   Total spatiotemporal cells: {len(final_df):,}\")\n",
    "print(f\"   Features: {list(final_df.columns)}\")\n",
    "print(f\"\\n   Data completeness summary:\")\n",
    "print(f\"   â€¢ SST coverage: {final_df['sst'].notna().sum():,} cells ({final_df['sst'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"   â€¢ Shark presence: {(final_df['shark_count'] > 0).sum():,} cells ({(final_df['shark_count'] > 0).mean()*100:.3f}%)\")\n",
    "print(f\"   â€¢ Prey presence: {(final_df['prey_count'] > 0).sum():,} cells ({(final_df['prey_count'] > 0).mean()*100:.2f}%)\")\n",
    "print(f\"   â€¢ Fishing activity: {(final_df['effort_hours'] > 0).sum():,} cells ({(final_df['effort_hours'] > 0).mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ae806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ENGINEERING: Derived Environmental Variables\n",
    "# =============================================================================\n",
    "# Transforms applied to raw SST and fishing effort to create ecologically\n",
    "# meaningful predictors for species distribution modeling.\n",
    "# =============================================================================\n",
    "print(\"Engineering derived features...\\n\")\n",
    "\n",
    "# 1. SST ANOMALIES: Deviation from local monthly climatology\n",
    "#    SST_anom = SST_ijt - mean(SST_ijm) for all years in study period\n",
    "monthly_clim = final_df.groupby(['month', 'lat', 'lon'])['sst'].transform('mean')\n",
    "final_df['sst_anom'] = final_df['sst'] - monthly_clim\n",
    "\n",
    "# 2. TEMPORAL LAGS: Memory effects in habitat selection\n",
    "#    SST_lag_k = SST_{ij,t-k} for k âˆˆ {1, 3} months\n",
    "final_df = final_df.sort_values(['lat', 'lon', 'date'])\n",
    "for lag in [1, 3]:\n",
    "    final_df[f'sst_lag{lag}'] = final_df.groupby(['lat', 'lon'])['sst'].shift(lag)\n",
    "    final_df[f'effort_lag{lag}'] = final_df.groupby(['lat', 'lon'])['effort_hours'].shift(lag)\n",
    "\n",
    "# 3. RATE OF CHANGE: Environmental variability signal\n",
    "#    Î”SST = SST_ijt - SST_{ij,t-1}\n",
    "final_df['sst_change'] = final_df.groupby(['lat', 'lon'])['sst'].diff()\n",
    "\n",
    "# 4. LATITUDINAL GRADIENT: Thermal front detection proxy\n",
    "#    âˆ‡_Ï† SST â‰ˆ (SST(Ï†+Î”Ï†) - SST(Ï†)) / Î”Ï†\n",
    "lat_gradient = final_df.groupby(['date', 'lon'])['sst'].transform(\n",
    "    lambda x: x.diff() / cfg.grid_deg if len(x) > 1 else 0\n",
    ")\n",
    "final_df['sst_lat_gradient'] = lat_gradient\n",
    "\n",
    "# 5. DISTANCE PROXY: Absolute latitude (correlates with distance to equator)\n",
    "final_df['dist_to_equator'] = np.abs(final_df['lat'])\n",
    "\n",
    "# 6. SEASONALITY ENCODING: Fourier basis for cyclic patterns\n",
    "#    sin/cos encoding prevents discontinuity at Decâ†’Jan transition\n",
    "final_df['month_sin'] = np.sin(2 * np.pi * final_df['month'] / 12)\n",
    "final_df['month_cos'] = np.cos(2 * np.pi * final_df['month'] / 12)\n",
    "\n",
    "# VALIDATION: Check anomaly distribution (should be centered ~0)\n",
    "anom_mean = final_df['sst_anom'].mean()\n",
    "assert abs(anom_mean) < 0.5, f\"SST anomaly mean too far from zero: {anom_mean:.2f}\"\n",
    "\n",
    "print(f\"âœ“ Derived features created:\")\n",
    "print(f\"   â€¢ SST anomalies: mean = {anom_mean:.3f}Â°C (expected ~0)\")\n",
    "print(f\"   â€¢ Temporal lags: 1-month, 3-month\")\n",
    "print(f\"   â€¢ Rate of change: monthly Î”SST\")\n",
    "print(f\"   â€¢ Spatial gradients: latitudinal SST gradient\")\n",
    "print(f\"   â€¢ Seasonality: sin/cos Fourier encoding\")\n",
    "print(f\"\\n   Feature statistics:\")\n",
    "print(f\"   â€¢ SST anomaly range: [{final_df['sst_anom'].min():.2f}, {final_df['sst_anom'].max():.2f}]Â°C\")\n",
    "print(f\"   â€¢ Max monthly SST change: {final_df['sst_change'].abs().max():.2f}Â°C\")\n",
    "print(f\"   â€¢ Latitudinal gradient range: [{final_df['sst_lat_gradient'].min():.3f}, {final_df['sst_lat_gradient'].max():.3f}] Â°C/Â°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b0d66c",
   "metadata": {},
   "source": [
    "## 6. Missing Data Handling & Quality Metrics\n",
    "\n",
    "**Strategy**: Given the extreme sparsity of shark occurrence data (presence/absence ratio ~1:1000), we retain all grid cells including zeros for proper class-imbalanced modeling.\n",
    "\n",
    "**Quality flags**:\n",
    "- `has_sst`: Binary indicator for SST data availability\n",
    "- `has_fishing`: Binary indicator for non-zero fishing effort  \n",
    "- `data_quality`: Composite score $Q = \\sum_k \\mathbb{1}[\\text{feature}_k \\neq \\text{missing}]$\n",
    "\n",
    "**Modeling subset**: Cells with `has_sst == 1` (environmental data required for prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MISSING DATA HANDLING & DATA QUALITY SCORING\n",
    "# =============================================================================\n",
    "# For sparse species data, we retain all cells (including absences) but flag\n",
    "# data quality to enable sensitivity analyses and complete-case subsets.\n",
    "# =============================================================================\n",
    "print(\"Flagging data quality and handling missing values...\\n\")\n",
    "\n",
    "# Create binary quality indicators\n",
    "final_df['has_sst'] = final_df['sst'].notna().astype(int)\n",
    "final_df['has_sst_lag1'] = final_df['sst_lag1'].notna().astype(int)\n",
    "final_df['has_fishing'] = (final_df['effort_hours'] > 0).astype(int)\n",
    "\n",
    "# Composite quality score: Q âˆˆ {0, 1, 2, 3} (higher = more complete)\n",
    "final_df['data_quality'] = (\n",
    "    final_df['has_sst'] + \n",
    "    final_df['has_sst_lag1'] + \n",
    "    final_df['has_fishing']\n",
    ")\n",
    "\n",
    "# Create modeling subset: require at minimum SST data (cannot predict without covariate)\n",
    "model_df = final_df[final_df['has_sst'] == 1].copy()\n",
    "\n",
    "# VALIDATION: Check that we retain sufficient positive class examples\n",
    "shark_positives_retained = (model_df['shark_presence'] == 1).sum()\n",
    "assert shark_positives_retained > 100, f\"Too few shark presences after filtering: {shark_positives_retained}\"\n",
    "\n",
    "print(f\"âœ“ Missing data handled:\")\n",
    "print(f\"   Total spatiotemporal cells: {len(final_df):,}\")\n",
    "print(f\"   Cells with SST (min. requirement): {(final_df['has_sst'] == 1).sum():,} ({(final_df['has_sst'] == 1).mean()*100:.1f}%)\")\n",
    "print(f\"   Cells with complete data (Q=3): {(final_df['data_quality'] == 3).sum():,} ({(final_df['data_quality'] == 3).mean()*100:.1f}%)\")\n",
    "print(f\"\\n   Modeling dataset (has_sst=1):\")\n",
    "print(f\"   â€¢ Cells retained: {len(model_df):,} ({len(model_df)/len(final_df)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Shark presences: {shark_positives_retained:,} ({(model_df['shark_presence'] == 1).mean()*100:.3f}%)\")\n",
    "print(f\"   â€¢ Mean quality score: {model_df['data_quality'].mean():.2f} / 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d8c368",
   "metadata": {},
   "source": [
    "## 7. Export Processed Datasets\n",
    "\n",
    "Save model-ready data in Parquet format (columnar, compressed) for downstream analysis:\n",
    "- **Full dataset**: All spatiotemporal cells including missing values (for imputation experiments)  \n",
    "- **Modeling dataset**: Complete-case subset with `has_sst == 1` (baseline analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE PROCESSED DATASETS\n",
    "# =============================================================================\n",
    "# Output format: Apache Parquet (columnar storage, efficient compression)\n",
    "# Two versions: (1) full with NAs, (2) complete cases only\n",
    "# =============================================================================\n",
    "print(\"Saving processed datasets to disk...\\n\")\n",
    "\n",
    "# 1. FULL DATASET (with missing values)\n",
    "full_path = PROCESSED_DIR / \"spatiotemporal_grid_full.parquet\"\n",
    "final_df.to_parquet(full_path, index=False)\n",
    "\n",
    "# VALIDATION: File size check\n",
    "file_size_mb = full_path.stat().st_size / 1024**2\n",
    "assert file_size_mb < 500, f\"Output file too large: {file_size_mb:.1f} MB (expected < 500 MB)\"\n",
    "\n",
    "print(f\"âœ“ Full dataset saved: {full_path.name}\")\n",
    "print(f\"   â€¢ File size: {file_size_mb:.1f} MB\")\n",
    "print(f\"   â€¢ Rows: {len(final_df):,}\")\n",
    "print(f\"   â€¢ Columns: {len(final_df.columns)}\")\n",
    "print(f\"   â€¢ Column names: {list(final_df.columns)}\")\n",
    "\n",
    "# 2. MODELING DATASET (complete cases for SST)\n",
    "model_path = PROCESSED_DIR / \"modeling_dataset.parquet\"\n",
    "model_df.to_parquet(model_path, index=False)\n",
    "\n",
    "model_size_mb = model_path.stat().st_size / 1024**2\n",
    "print(f\"\\nâœ“ Modeling dataset saved: {model_path.name}\")\n",
    "print(f\"   â€¢ File size: {model_size_mb:.1f} MB\")\n",
    "print(f\"   â€¢ Rows: {len(model_df):,} ({len(model_df)/len(final_df)*100:.1f}% of full dataset)\")\n",
    "print(f\"   â€¢ Columns: {len(model_df.columns)}\")\n",
    "\n",
    "# VALIDATION: Memory efficiency check\n",
    "memory_usage_mb = final_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\n   Memory footprint: {memory_usage_mb:.1f} MB (in-memory)\")\n",
    "print(f\"   Compression ratio: {memory_usage_mb / file_size_mb:.1f}x (memory/disk)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d075a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION: Feature Distribution Diagnostics\n",
    "# =============================================================================\n",
    "# Quick visual check of key feature distributions for data quality assessment\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Select key features for diagnostic visualization\n",
    "features_to_plot = ['sst', 'sst_anom', 'effort_hours', 'shark_count', 'prey_count', 'data_quality']\n",
    "\n",
    "for i, feat in enumerate(features_to_plot):\n",
    "    ax = axes[i]\n",
    "    data = model_df[feat].dropna()\n",
    "    \n",
    "    if feat in ['shark_count', 'prey_count', 'data_quality']:\n",
    "        # Discrete count variables â†’ bar plot\n",
    "        counts = data.value_counts().sort_index()\n",
    "        ax.bar(counts.index, counts.values, color='steelblue', edgecolor='k', alpha=0.7)\n",
    "    else:\n",
    "        # Continuous environmental variables â†’ histogram\n",
    "        ax.hist(data, bins=50, color='steelblue', edgecolor='k', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel(feat, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax.set_title(f'{feat} distribution (n={len(data):,})', fontweight='bold', pad=10)\n",
    "    ax.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROCESSED_DIR / '../figures/feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nâœ“ Feature distribution diagnostic plot saved: figures/feature_distributions.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nova-selachiia-oFZsEAWQ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
